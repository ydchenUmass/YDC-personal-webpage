[
  {
    "objectID": "storyboard.html",
    "href": "storyboard.html",
    "title": "Storyboard Commentary",
    "section": "",
    "text": "Frame 1\n\nSome commentary about Frame 1.\n\n\nFrame 2\n\nSome commentary about Frame 2."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html",
    "href": "Project2/challenge_4_Fall23_solutions.html",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#setup",
    "href": "Project2/challenge_4_Fall23_solutions.html#setup",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nlibrary(stringr) # if you have not installed this package, please install it.\nlibrary(ggplot2) # if you have not installed this package, please install it.\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#challenge-overview",
    "href": "Project2/challenge_4_Fall23_solutions.html#challenge-overview",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nIn this challenge, we will practice with the data we worked on in the previous challenges and the data you choose to do some simple data visualizations using the ggplot2 package.\nThere will be coding components and writing components. Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#datasets",
    "href": "Project2/challenge_4_Fall23_solutions.html#datasets",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Datasets",
    "text": "Datasets\n\nPart 1 the ESS_Polity Data (created in Challenge#3) ⭐⭐\nPart 2: the Australia Data⭐⭐\nPart 3: see Part 3. Practice plotting with a dataset of your choice (25% of the total grade). For online platforms of free data, see Appendix: sources for data to be used in Part 3.\n\nFind the _data folder, then read the datasets using the correct R command."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#part-1.-univariate-and-multivariate-graphs-45-of-the-total-grade",
    "href": "Project2/challenge_4_Fall23_solutions.html#part-1.-univariate-and-multivariate-graphs-45-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 1. Univariate and Multivariate Graphs (45% of the total grade)",
    "text": "Part 1. Univariate and Multivariate Graphs (45% of the total grade)\nWe have been working with these two data in the previous three challenges. Suppose we have a research project that studies European citizens’ social behaviors and public opinions, and we are interested in how the countries that respondents live in influence their behavior and opinion. In this challenge, let’s work with the combined dataset ESS_Polity and create some visualizations.\n\nRead the combined data you created last time. (2.5%)\n\n\n#read the data: \nESS_Polity &lt;- read_csv(\"ESS_Polity.csv\")\n\nSuppose we are interested in the central tendencies and distributions of the following variables. At the individual level: age, male, edu, income_10, and vote. At the country level: democ.\n(1) Recode the “vote” column: if the value is 1, recode it as 1; if the value is 2, recode it as 0; if the value is 3, recode it as NA. Make sure to include a sanity check for the recoded data. (2.5%)\n\n#recoding the vote column: 1.5%\nESS_Polity&lt;-ESS_Polity|&gt;\n  mutate(vote = case_when(\n    vote == 1 ~ 1,\n    vote == 2 ~ 0,\n    vote == 3 ~ NA,\n    TRUE ~ vote))\n\n#Sanity check for if vote is correctly coded: 1%\nunique(ESS_Polity$vote)\n\n[1] NA  0  1\n\n\n(2) For each of the five variables (age, edu, income_10, vote, and democ), please choose an appropriate type of univariate graph to plot the central tendencies and distribution of the variables. Explain why you choose this type of graph to present a particular variable (for example: “For example, I use a histogram to plot age because it is a continuous numeric variable”). (25%)\n(Note: You should use at least two types of univariate graphs covered in the lecture.)\nAnswer: First, I do summary statistics for the five variables to check their range, number of values, and measurement. This step would help me to determine which type of graph I should choose. This is optional, but strongly recommended when you are working on any data projects.\n\n##I am using a user-defined function created in Challenge#3, but you can use other descriptive statistics functions along with the baseR (such as summary()) or other packages.\n\nsum_stat &lt;- function(x){\n  stat &lt;- tibble(\n    range=range(x, na.rm = T),\n    mean=mean(x, na.rm = T),\n    sd=sd(x,na.rm=T),\n    na = sum(is.na(x)),\n    unique = length(unique(x)),\n    class = typeof(x)\n  )\n  return(stat)\n}\n\nsum_stat(ESS_Polity$age) #has 88 unique values and it is numeric (double): continous variable. Appropriate univariate graph type: boxplot, violin chart, density, histogram\n\n\n  \n\n\nsum_stat(ESS_Polity$edu) #has 4 unique values and it is numeric (double): continous or ordinal categorial variable (depends on if it is originally coded as the level of education or the years of education). Appropriate univariate graph type: histogram or barplot \n\n\n  \n\n\nsum_stat(ESS_Polity$income_10) #has 10 unique values and it is numeric (double):orordinal categorial variable (10 income groups, so can be treated as a contionus variable). Appropriate univariate graph type: boxplot, violin chart, histogram\n\n\n  \n\n\nsum_stat(ESS_Polity$vote)#has 2 unique values and it is numeric (double): binary variable (only 1 and 0). Appropriate univariate graph type: scatterplot or barplot\n\n\n  \n\n\nsum_stat(ESS_Polity$democ)#has 6 unique values and it is numeric (double): ordinal categorial variable (originally a contionus variable, but since there are less than 10 values, we can treat it as an ordinal categorical variable when plotting): bar plot \n\n\n  \n\n\n\nIf you only plotted the figure without explaining why you chose the specific type of figure, and if you chose an inappropriate type of figure for a variable: - 4%.\nIf you give explanations on why you chose the specific type of figure, and it is not an inappropriate type of figure for that variable: - 2%.\n\nAge:\n\n\n#For age, I choose a box plot. Noted that when we plot a boxplot, NA are automatically removed.\n\nage_boxplot&lt;-ggplot(ESS_Polity) +\n  geom_boxplot(aes(x = age), fill=\"slateblue\", alpha=0.2)+\n    labs(title = \"Distribution of Respondents' Age\",\n       x = \"Age\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n  \n\nage_boxplot\n\n\n\n\n\n\n\n\nOr I can do a histogram. Note that it is important to select an appropriate bin size. In this example, a bin size between 4 and 8 should show a similar distirbution pattern. If you use a different bin size: - 1%\n\n#Noted that when we plot a histogram, NA are automatically removed.\nage_hist&lt;-ggplot(ESS_Polity) +\n  geom_histogram(binwidth=4, aes(x = age), fill=\"slateblue\", alpha=0.2)+\n    labs(title = \"Distribution of Respondents' Age\",\n       x = \"Age\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n  \n\nage_hist\n\n\n\n\n\n\n\n\n\nEducation:\n\n\n#For edu, I can also choose a bar plot:\nedu_bar &lt;- ggplot(ESS_Polity, aes(x=edu)) + \n  geom_bar()+\n  labs(title = \"Distribution of Respondents' Levels of Education\",\n       x = \"Levels of Education\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\nedu_bar\n\n\n\n\n\n\n\n\nNoted that when we plot a bar plot, NAs are not automatically removed (the warning message showing 150). NA has no numerical meaning (and should not be ranked as the highest in visualization). Generally for plots showing data distribution, we should remove it from the graphs (if not removing NA in a barplot orf showing data distribution, we can remove it from the graphs (if not removing NA in a barplot or : -1%).\n\nedu_bar &lt;- ggplot(data = subset(ESS_Polity, !is.na(edu)), aes(x = as.factor(edu))) + \n  geom_bar()+\n  labs(title = \"Distribution of Respondents' Levels of Education\",\n       x = \"Levels of Education (na removed)\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n  \nedu_bar\n\n\n\n\n\n\n\n\nAs we can see, the shape of the bars is similar to the graph without removing NA, since there are few NAs in education.\n\nIncome:\n\n\n#For income groups, I can also choose a bar plot:\n\nincome_bar &lt;- ggplot(ESS_Polity, aes(x=as.factor(income_10))) + #I use as.factor to force R to recognize income_10 as an ordinal category, so that the x-axis tick mark labels can automatically represent all categories.\n  geom_bar(fill=\"#69b3a2\", color=\"#e9ecef\", alpha=0.8)+\n  labs(title = \"Distribution of Respondents' Income Levels\",\n       x = \"Income Levels (10 ordinal groups)\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n\nincome_bar\n\n\n\n\n\n\n\n\n\nIn this graph, we can see that a huge proportion of the income data is NAs. They actually affect the presentation by making the shape of the distribution not so obvious. Let’s remove them.\n\n\nincome_bar &lt;- ggplot(subset(ESS_Polity, !is.na(income_10)), aes(x=as.factor(income_10))) + \n  geom_bar(fill=\"#69b3a2\", color=\"#e9ecef\", alpha=0.8)+\n  labs(title = \"Distribution of Respondents' Income Levels\",\n       x = \"Income Levels (10 ordinal groups, na removed)\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n\nincome_bar\n\n\n\n\n\n\n\n\n\nVote:\n\n\n#For vote, I choose a bar plot.\n\nvote_bar &lt;- ggplot(subset(ESS_Polity, !is.na(vote)), aes(x=as.factor(vote))) + \n  geom_bar(fill=\"blue\", color=\"grey\", alpha=0.8)+\n  labs(title = \"Distribution of Respondents' Voter Turnout\",\n       x = \"Vote Choice (binary, na removed)\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")+\n  scale_x_discrete(labels=c(\"0\" = \"Didn't Vote\", \"1\" = \"Voted\"))\n\nvote_bar\n\n\n\n\n\n\n\n\n\nDemocracy:\n\n\n#For democ, I choose a bar plot.\n\ndemocracy_bar &lt;- ESS_Polity|&gt;\n  subset(!is.na(democ))|&gt;\n  ggplot(aes(x=as.factor(democ))) + \n  geom_bar(fill=\"red\", color=\"grey\", alpha=0.8)+\n  labs(title = \"Distribution of the Democracy Score of Countries by Respondents\",\n       x = \"Democracy Score (na removed)\",\n       y = \"Count of Respondents\", \n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n\ndemocracy_bar \n\n\n\n\n\n\n\n\n\nWe want to test two hypotheses on the relationships of two pairs of variables. Please use the appropriate type of graphs we learned to visualize these two pairs of variables. Briefly describe the graph you plot, and answer: Does the graph we create from the data support the hypothesis?\n\n\n\n(1) Hypothesis#1: The more years of education (edu) a person completed, the higher income (income_10) they earn. (7.5%)\nAnswer: edu is a variable with only four values. We can treat it as an ordinal categorical variable. Income has 10 values, and we can treat it as either a continuous or an ordinal categorical variable. In this case, we have several options: a stacked bar or a groupped bar.\n\n#don't forget to remove NAs (if not, -2%)\nedu_income&lt;-ESS_Polity|&gt;\n  subset(!is.na(income_10))|&gt; #remove na in income\n  subset(!is.na(edu))|&gt; #remove na in edu\n  ggplot(aes(x = as.factor(income_10), fill = as.factor(edu))) + \n    geom_bar()\n\nedu_income\n\n\n\n\n\n\n\n\nWe can clearly see as in the higher income group, the proportion of highest education level (4) increases. This is more obvious if we do a stacked percentage bar plot.\n\nedu_income&lt;-ESS_Polity|&gt;\n  subset(!is.na(income_10))|&gt; #remove na in income\n  subset(!is.na(edu))|&gt; #remove na in edu\n  ggplot(aes(x = as.factor(income_10), fill = as.factor(edu))) + \n    geom_bar(position=\"fill\")\n\nedu_income\n\n\n\n\n\n\n\n\nWe can also do a boxplot of income grouped (since income has 10 values, we can treat it as a continous variable) by education level:\n\nedu_income&lt;-ESS_Polity|&gt;\n  subset(!is.na(income_10))|&gt; #remove na in income\n  subset(!is.na(edu))|&gt; \n  ggplot(aes (x = as.factor(edu), y = income_10)) +\n  geom_boxplot(fill=\"slateblue\", alpha=0.2) \n\nedu_income\n\n\n\n\n\n\n\n\nHowever, scatter plots will look very strange and it is hard to estimate the pattern of variables with less than 10 values.\n\nedu_income&lt;-ESS_Polity|&gt;\n  subset(!is.na(income_10))|&gt; #remove na in income\n  subset(!is.na(edu))|&gt; \n  ggplot(aes(x = as.factor(income_10), y = as.factor(edu))) + \n  geom_point()+\n  geom_smooth()\n\nedu_income\n\n\n\n\n\n\n\n\nSo in conclusion, this hypothesis is supportive.\n(2) Hypothesis#2: There is a gender disparity (male) in voting behavior (vote). (Either men are more likely to vote, or women are more likely to vote). (7.5%)\nAnswer: both gender and vote are binary variables. So our option is either bar plots or .\n\nmale_vote&lt;-ESS_Polity|&gt;\n  subset(!is.na(male))|&gt; #remove na in income\n  subset(!is.na(vote))|&gt; \n  ggplot(aes(x = as.factor(male), fill = as.factor(vote))) + \n    geom_bar(position=\"fill\")\n\nmale_vote\n\n\n\n\n\n\n\n\n\nHmm, the average turnouts of male voters and female voters are very similar. It does seem that gender determines people’s voting decisions. In fact, if you use group_by and summarise(), you will find that one is 0.767 (female), and another is 0.759 (male). In conclusion, the second hypothesis does seem to be valid.\n\nESS_Polity|&gt;\n  group_by(male)|&gt;\n  subset(!is.na(male))|&gt; #remove na in income\n  subset(!is.na(vote))|&gt;\n  summarise(mean(vote))"
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#part-2.-comparing-between-partial-and-whole-and-among-groups-30-of-the-total-grade",
    "href": "Project2/challenge_4_Fall23_solutions.html#part-2.-comparing-between-partial-and-whole-and-among-groups-30-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 2. Comparing between Partial and Whole, and among Groups (30% of the total grade)",
    "text": "Part 2. Comparing between Partial and Whole, and among Groups (30% of the total grade)\nIn this part, we will use the clean version of the Australian public opinion poll on Same-Sex Marriage to generate graphs and plots. You may need to do the data transformation or mutation needed to help graphing.\n\nRead in data. (2.5%)\n\naustralian_data &lt;- read_csv(\"australian_data.csv\")\n\nUse a barplot to graph the Australian data based on their responses: yes, no, illegible, and no response. The y-axis should be the count of responses, and each response should be represented by one individual bar (so there should be four bars). (7.5%)\n(you can use either geom_bar() or geom_col())\nAnswer: First we need to reshape the data to convert it:\n\naus_long &lt;- australian_data |&gt;\n  pivot_longer(\ncols = Yes:`No Response`,\nnames_to = \"Response\",\nvalues_to = \"Count\"\n  )\nhead(aus_long)\n\n\n  \n\n\n\nAfter reshaping the data, we can plot it now:\n\n#Plot the barchart for repsonses:\n\nresponse_bar&lt;-ggplot(aus_long, aes(x=Response, y=Count))+\n  geom_bar(stat=\"identity\",fill=\"purple\")\n\nresponse_bar\n\n\n\n\n\n\n\n\nWe can customize the bar plot by reordering the bars, presenting the raw numbers of each response, and editing title and labels. We even change the y-axis tick marks from count to percentage (not required in the question).\n\n\nresponse_bar&lt;-aus_long|&gt;\n  mutate(Response = as_factor(Response), #we need to first force R to recognize \"Response\" as factor\n         Response = fct_relevel(Response, \"Yes\", \"No\", \"Illegible\"))|&gt; #then we can use fct_relvel to specify the order of the bars.\n  group_by(Response)|&gt;\n  summarise(Count = sum(Count))|&gt; # try without specifying group_by and ungroup, what do we got?\n  ungroup()|&gt; \n  mutate(perc = Count/sum(Count))|&gt;\n  ggplot(aes(y=perc, x=Response))+\n  geom_col()+\n  labs(title = \"The National Distribution of Resesponse\")+\n  scale_y_continuous(name= \"Percent of Citizens\", \n                     label = scales::percent) +\n  geom_text(aes(label = Count), size=3, vjust=-.5)\n  \nresponse_bar\n\n\n\n\n\n\n\n\n\nThe previous graph only shows the difference in amount. Let’s create a stacked-to-100% barplot to show the proportion of each of the four responses (by % of the total response). (7.5%)\n(you can use either geom_bar() or geom_col())\n\n#We will use the original data to plot this stacked-to-100% bar.\n\nresponse_stack&lt;-ggplot(aus_long, aes(fill = Response, x = '', y = Count)) + \n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(title = \"The National Distribution of Resesponse\", x = NULL, y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 100))\n\nresponse_stack\n\n\n\n\n\n\n\n\nLet’s see if there’s a relationship between Division and Response - that is, are certain divisions more likely to respond one way compared to other divisions? Again, we will use barplot(s) to present the visualization. (12.5%)\n(you can use either geom_bar() or geom_col())\n\ndivision_stack&lt;- ggplot(aus_long, aes(fill = Response, x = Division, y = Count)) + \n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(title = \"The National Distribution of Resesponse\", x = NULL, y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 100)) +\n  theme(axis.text.x=element_text(angle=60, hjust=1))\n\ndivision_stack\n\n\n\n\n\n\n\n\nMaybe it is difficult to see in stacked bars. Let’s plot regular side-by-side bar plots for each devision and use facet to combine multiple plots.\n\ndivision_facet&lt;-aus_long|&gt;\n  mutate(Response = as_factor(Response),\n         Response = fct_relevel(Response, \"Yes\", \"No\", \"Illegible\"),\n         Division=str_remove(Division,\" Divisions\")) |&gt;\n  group_by(Division,Response)|&gt;\n  summarise(Count = sum(Count))|&gt;\n  group_by(Division)|&gt;\n  mutate(perc = Count/sum(Count))|&gt;\n  ggplot(aes(y=perc, x=Response,fill=Response))+\n  geom_col()+\n  facet_wrap(vars(Division))+\n  labs(title = \"The Distribution of Resesponse by Division\") +\n  scale_y_continuous(name= \"Percent of Citizens\", \n                     label = scales::percent)+\n  theme(axis.text.x=element_text(angle = 60, hjust=1))\n\ndivision_facet"
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#part-3.-practice-plotting-with-a-dataset-of-your-choice-25-of-the-total-grade",
    "href": "Project2/challenge_4_Fall23_solutions.html#part-3.-practice-plotting-with-a-dataset-of-your-choice-25-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 3. Practice plotting with a dataset of your choice (25% of the total grade)",
    "text": "Part 3. Practice plotting with a dataset of your choice (25% of the total grade)\nIn this part, you will choose data of your interests for graphing and plotting. This data can be tidy/ready-to-be-used or raw data that needs cleaning. If the data is very large (for example, more than 20 columns), you should definitely subset the data by selecting less than 10 variables of your interests to avoid taking too much room in your R memory.\n\nInclude a link to the data page (this page should include the introduction or description and the link to download this dataset). (2%)\nRead the data you choose and briefly answer the following questions. (Optional: you may need to subset, clean, and transform the data if necessary). (8%)\n\n#type of your code/command here.\n\n(1) what is the structure (dimension) of the data;\n(2) what is the unit of observation?\n(3) what does each column mean in this data?\nChoose two columns/variables of your interests. Plot one univariate graph for each of the variables. (5%)\n\n#type of your code/command here.\n\n\n\n\nChoose a pair of variables you suspect or hypothesize may be correlated and a graph (scatter plot or barplot) using them. Based on the visual evidence, do you see any potential correlation between the two variables (10%)\n\n#type of your code/command here."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#appendix-sources-for-data-to-be-used-in-part-3",
    "href": "Project2/challenge_4_Fall23_solutions.html#appendix-sources-for-data-to-be-used-in-part-3",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Appendix: sources for data to be used in Part 3",
    "text": "Appendix: sources for data to be used in Part 3\nHere are some online sources and popular Online Dataset Hub:\n\nMany US governments (usually at the federal and state levels),  bureaus, and departments have open data archives on their websites, allowing the public to access, download, and use them. Just use Google to search for them.\n\n\n\nThe Harvard Dataverse Repository is a free data repository open to all researchers from any discipline, inside and outside the Harvard community, where you can share, archive, cite, access, and explore research data. Each individual Dataverse collection is a customizable collection of datasets (or a virtual repository) for organizing, managing, and showcasing datasets.\n\n\n\nInter-university Consortium for Political and Social Research (ICPSR) of the University of Michigan-Ann Arbor provides leadership and training in data access, curation, and methods of analysis for the social science research community. \n\n\n\nUN: https://data.un.org/\n\n\n\nOECD Data:  economic and development data of the most developed countries in the world.\n\n\n\nThe upper five sources are mainly for social science data; there is another very big community and open data archives for machine-learning and data science: Kaggle."
  },
  {
    "objectID": "multiple_pages.html",
    "href": "multiple_pages.html",
    "title": "Multiple Pages",
    "section": "",
    "text": "#read the data: \nstarbucks &lt;- read_csv(\"https://raw.githubusercontent.com/libjohn/mapping-with-R/master/data/All_Starbucks_Locations_in_the_US_-_Map.csv\", show_col_types = FALSE)\n#head(starbucks)\n\n##Create a column of \"gross profit\" and assign random numbers to it:\nstarbucks &lt;- starbucks |&gt;\n  mutate(gross_profit = sample(1000:10000, size = n(), replace = TRUE))\n#head(starbucks)\n\nstarbucksMA &lt;- starbucks  |&gt;\n  filter(State %in% c(\"MA\"))\n\nstarbucks_sf &lt;- starbucksMA |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nmapview(starbucks_sf, col.regions = \"red\", cex = \"gross_profit\", legend = TRUE, map.types = \"OpenStreetMap\")\n\n\n\n\n#col.region: Sets all bubbles to a fixed color\n#cex:sets the bubble size to be proportional to a specific column\n\n\n\n\n\n\n\nSource: Starbucks Location in the US\nIn this visualization, we subset the data by keeping only starbucks in Massachusetts.\n\n\n\nStarbucks are more condensed in urban area, such as the greater Boston and Springfield."
  },
  {
    "objectID": "multiple_pages.html#column",
    "href": "multiple_pages.html#column",
    "title": "Multiple Pages",
    "section": "",
    "text": "#read the data: \nstarbucks &lt;- read_csv(\"https://raw.githubusercontent.com/libjohn/mapping-with-R/master/data/All_Starbucks_Locations_in_the_US_-_Map.csv\", show_col_types = FALSE)\n#head(starbucks)\n\n##Create a column of \"gross profit\" and assign random numbers to it:\nstarbucks &lt;- starbucks |&gt;\n  mutate(gross_profit = sample(1000:10000, size = n(), replace = TRUE))\n#head(starbucks)\n\nstarbucksMA &lt;- starbucks  |&gt;\n  filter(State %in% c(\"MA\"))\n\nstarbucks_sf &lt;- starbucksMA |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nmapview(starbucks_sf, col.regions = \"red\", cex = \"gross_profit\", legend = TRUE, map.types = \"OpenStreetMap\")\n\n\n\n\n#col.region: Sets all bubbles to a fixed color\n#cex:sets the bubble size to be proportional to a specific column"
  },
  {
    "objectID": "multiple_pages.html#column-1",
    "href": "multiple_pages.html#column-1",
    "title": "Multiple Pages",
    "section": "",
    "text": "Source: Starbucks Location in the US\nIn this visualization, we subset the data by keeping only starbucks in Massachusetts.\n\n\n\nStarbucks are more condensed in urban area, such as the greater Boston and Springfield."
  },
  {
    "objectID": "multiple_pages.html#row",
    "href": "multiple_pages.html#row",
    "title": "Multiple Pages",
    "section": "Row",
    "text": "Row\n\nChart 1"
  },
  {
    "objectID": "multiple_pages.html#row-1",
    "href": "multiple_pages.html#row-1",
    "title": "Multiple Pages",
    "section": "Row",
    "text": "Row\n\nChart 2\n\n\nChart 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yongdong Chen",
    "section": "",
    "text": "Welcome to my website! I am a PhD student in Operations Management in UMass Amherst. My research interest is sustainable operations management.\nYou can find more details in my Curriculum Vitae(FileSample).\n\n\n\n\nPh.D. Operations Management, UMass Amherst 2024.09-\nM.A. Business Administration, Central University of Finance and Economics\nB.A. Logistics Management, Central University of Finance and Economics\n\n\n\n\nI have worked on several data-driven projects. Please verify the Project page for more details.\nThank you for visiting"
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "About Me",
    "section": "",
    "text": "Teach the class to host webpage and projects using GitHub."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Yongdong Chen",
    "section": "",
    "text": "Ph.D. Operations Management, UMass Amherst 2024.09-\nM.A. Business Administration, Central University of Finance and Economics\nB.A. Logistics Management, Central University of Finance and Economics"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Dong Yu (Erico)",
    "section": "",
    "text": "Before joining DACSS, I taught in the Department of Political Science at Grinnell College. I also served as a research intern at the Institute for Policy Studies in Washington, D.C., and The Carter Center in Atlanta. Additionally, I worked as an assistant in a law clinic at Sun Yat-sen University.\nBeyond academia, I contributed to a Podcast on American Politics and Elections and wrote for various media outlets and newspapers, including The New York Times, covering US presidential and midterm elections."
  },
  {
    "objectID": "flexboard.html",
    "href": "flexboard.html",
    "title": "Sustainability Dashboard",
    "section": "",
    "text": "if(nrow(plot_data) &gt; 0) {\n  \n  ggplot(plot_data, aes(x = Year, y = Count, fill = fct_rev(AuditorLabel))) +\n    geom_col(width = 0.65) + \n    \n    scale_fill_manual(values = c(\"Not Audited\" = \"#A5D6A7\", \"Audited\" = \"#2E8B57\")) +\n    \n    scale_x_continuous(breaks = seq(min(plot_data$Year), max(plot_data$Year), 1)) +\n    \n    scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n    \n    theme_minimal(base_size = 14) +\n    theme(\n      legend.position = \"top\", \n      legend.justification = \"left\",\n      legend.title = element_blank(),\n      \n      panel.grid.major.x = element_blank(),\n      panel.grid.minor = element_blank(),\n      \n      plot.title = element_text(face = \"bold\", size = 18),\n      plot.subtitle = element_text(size = 12, color = \"grey30\")\n    ) +\n    \n    labs(\n      title = \"The Sustainability Reporting Boom\",\n      subtitle = \"Number of S&P 500 firms publishing sustainability reports\",\n      x = \"\",\n      y = \"Number of Companies\"\n    )\n    \n} else {\n  print(\"Error: No data found.\")\n}"
  },
  {
    "objectID": "Input Sidebar.html",
    "href": "Input Sidebar.html",
    "title": "Sidebar",
    "section": "",
    "text": "# shiny inputs defined here"
  },
  {
    "objectID": "Input Sidebar.html#inputs",
    "href": "Input Sidebar.html#inputs",
    "title": "Sidebar",
    "section": "",
    "text": "# shiny inputs defined here"
  },
  {
    "objectID": "Input Sidebar.html#column",
    "href": "Input Sidebar.html#column",
    "title": "Sidebar",
    "section": "Column",
    "text": "Column\n\nChart 1\n\n\nChart 2"
  },
  {
    "objectID": "Project1/starbucks.html",
    "href": "Project1/starbucks.html",
    "title": "Interactive Map",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "Project1/starbucks.html#setup",
    "href": "Project1/starbucks.html#setup",
    "title": "Interactive Map",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nlibrary(stringr) # if you have not installed this package, please install it.\nlibrary(ggplot2) # if you have not installed this package, please install it.\nlibrary(sf) \nlibrary(mapview)\n#Loading colorblind-friendly color map package: viridisLite\nlibrary(viridis)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "Project1/starbucks.html#part-1.-starbucks-map",
    "href": "Project1/starbucks.html#part-1.-starbucks-map",
    "title": "Interactive Map",
    "section": "Part 1. Starbucks Map",
    "text": "Part 1. Starbucks Map\n\nLoading the example data of starbuck locations\n\n\n#read the data: \nstarbucks &lt;- read_csv(\"https://raw.githubusercontent.com/libjohn/mapping-with-R/master/data/All_Starbucks_Locations_in_the_US_-_Map.csv\", show_col_types = FALSE)\nhead(starbucks)\n\n\n  \n\n\n##Create a column of \"gross profit\" and assign random numbers to it:\nstarbucks &lt;- starbucks |&gt;\n  mutate(gross_profit = sample(1000:10000, size = n(), replace = TRUE))\n\nhead(starbucks)\n\n\n  \n\n\n\nsubset locations data to MA\n\nstarbucksMA &lt;- starbucks  |&gt;\n  filter(State %in% c(\"MA\"))\n\nwrite.csv(starbucksMA, \"starbucksMA.csv\", row.names = FALSE)\n\nConvert the dataset into a spatial object (sf)\n\nstarbucks_sf &lt;- starbucksMA |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nlet’s compare the difference between using size and brightness\nSize:\n\nmapview(starbucks_sf, col.regions = \"red\", cex = \"gross_profit\", legend = TRUE, map.types = \"OpenStreetMap\")\n\n\n\n\n#col.region: Sets all bubbles to a fixed color\n#cex:sets the bubble size to be proportional to a specific column\n\nBrightness:\n\nmapview(starbucks_sf, zcol = \"gross_profit\", cex = 3, legend = TRUE, map.types = \"OpenStreetMap\")\n\n\n\n\n#zcol = specifies that the colors (brightness or color/hue depends on if the variable is continous or not) of the bubbles should vary by the values of a particular column"
  },
  {
    "objectID": "Project1/starbucks.html#part-2.-animated-graph-mtcars",
    "href": "Project1/starbucks.html#part-2.-animated-graph-mtcars",
    "title": "Interactive Map",
    "section": "Part 2. Animated Graph (mtcars)",
    "text": "Part 2. Animated Graph (mtcars)\n\nlibrary(gganimate)\nlibrary(gifski)\n\n# Example animation\np &lt;- ggplot(mtcars, aes(mpg, wt)) +\n  geom_point() +\n  transition_states(gear, transition_length = 2, state_length = 1) +\n  ggtitle('Gear: {closest_state}')\n\n# Use gifski_renderer for GIF animations\nanimate(p, renderer = gifski_renderer())\n\n\n\n\n\n\n\n# You can try different setup, such as duration, fps, size of the graph):\n#animate(p, duration = 25, fps = 10, width = 1000, height = 1000, renderer = #gifski_renderer())"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Teaching",
    "section": "",
    "text": "Below are syllabi and teaching materials for the recent courses I teach at UMass Amherst:"
  },
  {
    "objectID": "projects.html#syllabus-pdf-file",
    "href": "projects.html#syllabus-pdf-file",
    "title": "Projects",
    "section": "Syllabus (PDF file)",
    "text": "Syllabus (PDF file)\nOpen Syllabus"
  },
  {
    "objectID": "projects.html#solutions-a-standalone-.html-file",
    "href": "projects.html#solutions-a-standalone-.html-file",
    "title": "Projects",
    "section": "Solutions (A standalone .html file)",
    "text": "Solutions (A standalone .html file)\nOpen the Standalone Page"
  },
  {
    "objectID": "projects.html#a-project-with-live-map-and-animation",
    "href": "projects.html#a-project-with-live-map-and-animation",
    "title": "Projects",
    "section": "A Project with live map and animation;",
    "text": "A Project with live map and animation;\nProject 1"
  },
  {
    "objectID": "projects.html#a-project-with-data-and-.qmd-file",
    "href": "projects.html#a-project-with-data-and-.qmd-file",
    "title": "Projects",
    "section": "A Project with data and .qmd file",
    "text": "A Project with data and .qmd file\nProject 2"
  },
  {
    "objectID": "projects.html#a-dashboard-example",
    "href": "projects.html#a-dashboard-example",
    "title": "Projects",
    "section": "A Dashboard Example",
    "text": "A Dashboard Example\nOpen Dashboard"
  },
  {
    "objectID": "projects.html#dacss-601-data-science-fundamentals",
    "href": "projects.html#dacss-601-data-science-fundamentals",
    "title": "Teaching",
    "section": "DACSS-601: Data Science Fundamentals",
    "text": "DACSS-601: Data Science Fundamentals\nThis course provides an essential introduction to R programming, designed for students with no prior programming or statistical experience. As a foundational tool in the DACSS program, R is widely used across core courses and technical electives. The course covers fundamental skills in data management, wrangling, and visualization, preparing students for advanced quantitative analysis. Beyond technical proficiency, it emphasizes data-driven storytelling, fostering technological and data literacy to enhance analytical thinking, argumentation, and communication. Students will learn to interpret data effectively and create compelling tables and figures to support their arguments.\n\nOpen Syllabus\nGoogle Colab Tutorials for R Basics and Data Analysis (Example)\nHomework Solutions(Example)"
  },
  {
    "objectID": "projects.html#dacss-603-introduction-to-quantitative-analysis",
    "href": "projects.html#dacss-603-introduction-to-quantitative-analysis",
    "title": "Teaching",
    "section": "DACSS-603: Introduction to Quantitative Analysis",
    "text": "DACSS-603: Introduction to Quantitative Analysis\nThis course provides a rigorous introduction to quantitative empirical research methods, designed for doctoral students in the social sciences and master’s students in data analytics or computational social science. It covers probability, hypothesis testing, and regression analysis, with a strong focus on simple and multiple linear regression, as well as logistic regression and generalized linear models. The course also critically examines conventional statistical practices, addressing issues such as the replication crisis, p-values, and limitations of null hypothesis testing. Students will engage in simulations and data analysis using R, developing both technical proficiency and a deeper understanding of statistical methodologies.\n\nOpen Syllabus"
  },
  {
    "objectID": "projects.html#dacss-604-advanced-data-driven-storytelling",
    "href": "projects.html#dacss-604-advanced-data-driven-storytelling",
    "title": "Teaching",
    "section": "DACSS-604: Advanced Data-driven Storytelling",
    "text": "DACSS-604: Advanced Data-driven Storytelling\nThis hands-on course provides students with the knowledge and skills needed to generate strong, data-driven communication. We will emphasize graphic and visualization theory and practices, clear interpretation of data patterns, quantitative analyses, and statistical inferences, using precise and accessible language appropriate for one’s audience.\n\nOpen Syllabus\nDashboard for Class Demo\nLive Map and Animated GIF for Class Demo"
  },
  {
    "objectID": "projects.html#legal-393e-empirical-legal-studies",
    "href": "projects.html#legal-393e-empirical-legal-studies",
    "title": "Teaching",
    "section": "Legal-393E: Empirical Legal Studies",
    "text": "Legal-393E: Empirical Legal Studies\nFrom academic research on the impact of landmark court cases to the judge reading a brief loaded with social policy arguments, the study of law frequently involves empirical data. In this course, students will both learn how to digest empirical legal research and be equipped with the skills to conduct their own empirical research. Using readings and accompanying data from a variety of areas of legal studies research, the course covers topics including constructing empirical legal research questions, collecting and generating data appropriate for testing those questions, visualizing data, and conducting descriptive and inferential analyses. Students will leave the course with a firm understanding of the centrality of data in twenty-first-century law.\n\nOpen Syllabus\nQuiz Solutions and Additional Teaching Notes:\n\nTutorial 1: R Basics"
  },
  {
    "objectID": "projects.html#other-teaching-materials-politics-of-china",
    "href": "projects.html#other-teaching-materials-politics-of-china",
    "title": "Teaching",
    "section": "Other Teaching Materials: Politics of China",
    "text": "Other Teaching Materials: Politics of China"
  },
  {
    "objectID": "Legal393E_Notes/Lab1.html",
    "href": "Legal393E_Notes/Lab1.html",
    "title": "Legel 393E Spring 2025: Lab Meeting#1: Tutorial#1 Quiz Solutions & More on R Basics",
    "section": "",
    "text": "As I introduced in class, the RStudio interface has four panels/panes. You can enter the R codes you learn and get them running in two ways. First, you can directly type in the codes in the R console pane and hit “enter,” and the immediate output will display below.\nSecond, in the source/script pane, you can create a new script (.r) by selecting “File/New File/R Script.” You can enter your codes in this script file. Once you are done, you can select one or multiple lines and hit the “Run” button on the upper right of the pane. The codes you run, and the outputs will be displayed on the R console pane. If you plot graphs or figures, the output will appear in the “Plots” window on the Output pane at the bottom right.\nIn practice, we generally use the second way to work on codes and analyze data in R. This .r script file is like a notebook or .txt file (if you have some programming background before). It allows you to write and edit multiple lines of code, write short functions or programs, make notes for your codes, and save all these codes and notes in this .r script file. You can also share this .r script file with others or open one existing .r script file in your RStudio and run the codes. It is very convenient to manage a project with multiple parts of code (reading data, processing data, coding and recoding, summary statistics, running models and getting results, creating visualizations, etc.)."
  },
  {
    "objectID": "Legal393E_Notes/Lab1.html#rstudio-layout-inputting-and-running-r-codes.",
    "href": "Legal393E_Notes/Lab1.html#rstudio-layout-inputting-and-running-r-codes.",
    "title": "Legel 393E Spring 2025: Lab Meeting#1: Tutorial#1 Quiz Solutions & More on R Basics",
    "section": "",
    "text": "As I introduced in class, the RStudio interface has four panels/panes. You can enter the R codes you learn and get them running in two ways. First, you can directly type in the codes in the R console pane and hit “enter,” and the immediate output will display below.\nSecond, in the source/script pane, you can create a new script (.r) by selecting “File/New File/R Script.” You can enter your codes in this script file. Once you are done, you can select one or multiple lines and hit the “Run” button on the upper right of the pane. The codes you run, and the outputs will be displayed on the R console pane. If you plot graphs or figures, the output will appear in the “Plots” window on the Output pane at the bottom right.\nIn practice, we generally use the second way to work on codes and analyze data in R. This .r script file is like a notebook or .txt file (if you have some programming background before). It allows you to write and edit multiple lines of code, write short functions or programs, make notes for your codes, and save all these codes and notes in this .r script file. You can also share this .r script file with others or open one existing .r script file in your RStudio and run the codes. It is very convenient to manage a project with multiple parts of code (reading data, processing data, coding and recoding, summary statistics, running models and getting results, creating visualizations, etc.)."
  },
  {
    "objectID": "Legal393E_Notes/Lab1.html#tutorial1-quiz-answers",
    "href": "Legal393E_Notes/Lab1.html#tutorial1-quiz-answers",
    "title": "Legel 393E Spring 2025: Lab Meeting#1: Tutorial#1 Quiz Solutions & More on R Basics",
    "section": "Tutorial#1 Quiz Answers",
    "text": "Tutorial#1 Quiz Answers\nQuestion 1. How does R evaluate (21+19)/2 + 1 &gt; 20?\nAnswer: We can simply enter the inequation and see the output.\n\n (21+19)/2 + 1 &gt; 20\n\n[1] TRUE\n\n\nQuestion 2. Which of the following is the assignment operator?\nAnswer: &lt;-.\nQuestion 3. What is the highest grade (as a percentage) in new_grades?\nAnswer: This question is based on the materials in the “Vectors” section of Tutorial#1. The vector new_grades is generated as below.\nNote: You can use this operator, “#,” in tutorials and many R coding blocks. This operator tells R not to run the content after it and to move on to the next, so it is a way to provide line-by-line commentary without interrupting R’s ability to run the script.\n\n#The second quiz\nnew_scores &lt;- c(13,12,11,9,12,13,15,12,6,14)\nnew_scores\n\n [1] 13 12 11  9 12 13 15 12  6 14\n\n#Solution of Exercise 1\n\n#new_grades variable is calculated dividing the scores by 15\nnew_grades &lt;- new_scores/15\n#multiply new_grades by 100 for clean percentages\nnew_grades &lt;- new_grades*100\n\n#preview all the numbers of \"new_grades\"\nnew_grades\n\n [1]  86.66667  80.00000  73.33333  60.00000  80.00000  86.66667 100.00000\n [8]  80.00000  40.00000  93.33333\n\n\nHow can we extract the maximum from this vector or a string of 10 numbers? We can simply use a basic R function, max():\n\n#getting the maximum number \nmax(new_grades)\n\n[1] 100\n\n\nQuestion#4. What is the highest grade (as a percentage) in final_grades after weighting by adding 5 points to everyone’s grade?\nAnswer: we first need to take the average of the sum of grades of the two quizzes to get the final_grades.\n\n#getting the grades of the first quiz. This is the same codes shown in the first part of the \"Vector\" section of the tutorials.\n\n#The first quiz\nscores &lt;- c(14,13,12,15,16,14,15,10,8,12) #creating a vector of 10 students' grades of Quiz 1 by c(), and assign it as variable \"scores\".\n\n#Creating a second variable called \"grades\" by dividing the scores by 17\ngrades &lt;- scores/17\n\n#convert it to percentage\ngrades &lt;- grades *100\n\n#preview all the numbers of \"grades\"\ngrades\n\n [1] 82.35294 76.47059 70.58824 88.23529 94.11765 82.35294 88.23529 58.82353\n [9] 47.05882 70.58824\n\n#now we can create the variable, \"final_grades\", by averaging the sum of the two quizzes:\nfinal_grades &lt;- (grades+new_grades)/2\n\n#show final_grades to dobule-check\nfinal_grades\n\n [1] 84.50980 78.23529 71.96078 74.11765 87.05882 84.50980 94.11765 69.41176\n [9] 43.52941 81.96078\n\n\nFinally, we just need to weight all the final grades by adding 5 points.\n\n#weight the grades by 5 points\nweighted_grades &lt;- final_grades + 5\n\n#What is the highest grade? Again, use the max() function.\nmax(weighted_grades)\n\n[1] 99.11765\n\n\nSurely, when you are more familiar with coding in R, you can rewrite the above multiple lines of codes into one line (just like merging multiple steps of calculation) and make your codes more clean and concise. However, you may still want to do this step by step to make sure the result of each step is correct.\n\nmax((scores/17 + new_scores/15)*100/2+5)\n\n[1] 99.11765\n\n\nQuestion.5 In R, c(1,2,3,4) would create:\nAnswer: This one is simple. A vector. If you want to get the sum of these four numbers using the following mutliple ways:\n\n1+2+3+4\n\n[1] 10\n\n#or\nvector &lt;- c(1,2,3,4)\nsum(vector)\n\n[1] 10\n\n#or\nsum(1:4)\n\n[1] 10\n\n#or\nsum(1,2,3,4)\n\n[1] 10"
  },
  {
    "objectID": "Legal393E_Notes/Lab1.html#additional-introduction-to-r-basic-operations",
    "href": "Legal393E_Notes/Lab1.html#additional-introduction-to-r-basic-operations",
    "title": "Legel 393E Spring 2025: Lab Meeting#1: Tutorial#1 Quiz Solutions & More on R Basics",
    "section": "Additional Introduction to R Basic Operations",
    "text": "Additional Introduction to R Basic Operations\nAlthough this is not an R coding course, I would suggest you do more research to learn R, including reading this suggested textbook. Besides providing your the tutorials and the lab days, i will also provide some additional notes on R coding practices.\nThis week, I want to introduce you to three things: working directory, packages, and functions. For each part, I attach the link of the readings for your to refer to.\n\nWorking Directory: Readings\n\nFirst, let’s start with some simple tasks to refresh the basics of R & RStudio. 1.Check your working directory (type your code below and run it)\n\n\ngetwd()\n\n[1] \"C:/Users/donge/OneDrive/Desktop/GitHub/webpage/Legal393E_Notes\"\n\n\n\nYou can use setwd() to change your working directory to any location in your device, such as desktop, for example. (for students using Posit Cloud, change the working directory to /cloud/)\n\nIf see a warning message, please refer to this answer on Piazza to see the solution.\n\n\nPackages; Installing & Loading Packages\nPackages in R Programming language are a set of R functions, compiled code, and sample data. These are stored under a directory called “library” within the R environment. By default, R installs a group of packages during installation. Once we start the R console, only the default packages are available by default. Other packages that are already installed need to be loaded explicitly to be utilized by the R program that’s getting to use them.\nR is an open-source programming language, which means that everyone can write their own packages.\nThere are two ways of installing packages. Let’s try the less intuitive way by using the function install.packages(). Say, if we want to install package “foreign,” how can you do it?\n\n#install.packages('foreign') #delete the previous \"#\" to run\n\npros: flexible; can install multiple packages per time; cons: less intuitive; you need to know the names of the packages you want to install.\nloading a package (always make sure do this first)\n\nlibrary(foreign)\n\nWarning: package 'foreign' was built under R version 4.4.2\n\n\nQuestion: is there a way that we can load multiple packages (tidyverse, ggplot2, foreign) at the same time?\n\nneeded.packages&lt;-c('tidyverse','ggplot2','haven')\nlapply(needed.packages, require, character.only = TRUE)\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: haven\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n\nWe can use this R base function: lapply(), which applies a function over a list or vector. In this case, we first assign a list of characters including all the packages’ names. Then we use lapply() to go over this list.\n\n#What if the package is not installed? \n# Install packages not yet installed\n#installed_packages &lt;- packages %in% rownames(installed.packages())\n#if (any(installed_packages == FALSE)) {\n#  install.packages(packages[!installed_packages])\n#}\n\n\n\nFunctions: Reading\nA function is a block of code that only runs when it is called. In R, many functions are already built-in and ready to call and run. Different functions are also included in other packages. When you install and load the packages, they are also ready to be used.\nIn the Tutorial#1 Quiz Answers above, I have shown you one of the default function in R. functions: max().\nLet’s try to do some practices with some built-in functions.\n\n#Creating a vector of 15 users' ratings of Iphone 15.\n\nrating_iphone15 &lt;- sample(1:10, 50, replace = T) # replace = T allow duplicated numbers\n\n\n#Creating a vector of the age of death of US presidents born before 1900.\n\nPrezAge_1900 &lt;- c(67, 90, 83, 85, 73, 80,\n                  78, 79, 68, 71, 53, 65,\n                  74, 64, NA, 56, 49, 57,\n                  71, 67, 71, 58, 60, 72,\n                  67, 57, 60, 90, 63, 88,\n                  78)\n\n\nPlease calculate the following basic stasitics of the above two vectors: mean, max, min, and median\n\n\nmean(rating_iphone15)\n\n[1] 5.28\n\nmin(rating_iphone15)\n\n[1] 1\n\nmax(rating_iphone15)\n\n[1] 10\n\nmedian(rating_iphone15)\n\n[1] 5\n\n\n\nmean(PrezAge_1900, na.rm = T) #na.rm is an argument in R, meaning that we want to remove the Non applicable data or or null data in the vector. So in this function, we tell R to calcualte the mean of this string of numbers while ignoring all the NAs.  \n\n[1] 69.8\n\nmin(PrezAge_1900, na.rm = T)\n\n[1] 49\n\nmax(PrezAge_1900, na.rm = T)\n\n[1] 90\n\nmedian(PrezAge_1900, na.rm = T)\n\n[1] 69.5\n\n\n\nIn base R, there is a built-in function we use a lot for all the above descriptive statistics: summary(). Try it with the above two vectors.\n\n\nsummary(PrezAge_1900)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  49.00   60.75   69.50   69.80   78.00   90.00       1 \n\nsummary(rating_iphone15)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    4.00    5.00    5.28    7.00   10.00 \n\n\nQuestion: what about the modes(the most frequent number in a sequence) of these vectors? Does mode() help us to get the number?\n\n#Note that R does not have a built-in function to calculate mode. Although we don't ususally show mode in data analysis, if we have to get the mode, we will need to write our own codes.\n\ngetmode&lt;-function(vector){\n  unique_value&lt;-unique(vector)# Find unique values in the input vector by removing all duplicated elements\n  counts &lt;-tabulate(match(vector, unique_value))  # Create a table of counts for each unique value in the vector\n  mode_index&lt;-which.max(counts) #which.max(): determine the location or index of the element (numeric) of the maximum value  \n  mode&lt;-unique_value[mode_index] #Return the unique value corresponding to the mode\n  return(mode)\n} \n\n#So in the above blocks of code, I actually writing my own function, using the function \"function()\". After I define the function, getmode(), I can just use it to get the modes of a string of numbers! For example:\n\n\nNext, try a few useful base functions with these two vectors: sort(), unique(), and table().\n\n\nsort(rating_iphone15) #this helps to sort the numbers (in ascending order by defaut)\n\n [1]  1  1  1  1  1  2  2  2  3  3  3  4  4  4  4  4  4  4  4  4  4  4  4  4  5\n[26]  5  5  5  6  6  6  6  6  7  7  7  7  7  8  8  8  8  9  9  9  9  9 10 10 10\n\nsort(rating_iphone15, decreasing = T)#and we can make it in a descending order\n\n [1] 10 10 10  9  9  9  9  9  8  8  8  8  7  7  7  7  7  6  6  6  6  6  5  5  5\n[26]  5  4  4  4  4  4  4  4  4  4  4  4  4  4  3  3  3  2  2  2  1  1  1  1  1\n\nunique(rating_iphone15) #unique returns a vector, data frame or array like x but with duplicate elements/rows removed.\n\n [1]  6  2  4  3  9 10  5  8  7  1\n\ntable(rating_iphone15)\n\nrating_iphone15\n 1  2  3  4  5  6  7  8  9 10 \n 5  3  3 13  4  5  5  4  5  3"
  },
  {
    "objectID": "Teaching.html",
    "href": "Teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Below are syllabi and teaching materials for the recent courses I teach at UMass Amherst:"
  },
  {
    "objectID": "Teaching.html#dacss-601-data-science-fundamentals",
    "href": "Teaching.html#dacss-601-data-science-fundamentals",
    "title": "Teaching",
    "section": "DACSS-601: Data Science Fundamentals",
    "text": "DACSS-601: Data Science Fundamentals\nThis course provides an essential introduction to R programming, designed for students with no prior programming or statistical experience. As a foundational tool in the DACSS program, R is widely used across core courses and technical electives. The course covers fundamental skills in data management, wrangling, and visualization, preparing students for advanced quantitative analysis. Beyond technical proficiency, it emphasizes data-driven storytelling, fostering technological and data literacy to enhance analytical thinking, argumentation, and communication. Students will learn to interpret data effectively and create compelling tables and figures to support their arguments.\n\nOpen Syllabus\nGoogle Colab Tutorials for R Basics and Data Analysis (Example)"
  },
  {
    "objectID": "Teaching.html#dacss-603-introduction-to-quantitative-analysis",
    "href": "Teaching.html#dacss-603-introduction-to-quantitative-analysis",
    "title": "Teaching",
    "section": "DACSS-603: Introduction to Quantitative Analysis",
    "text": "DACSS-603: Introduction to Quantitative Analysis\nThis course provides a rigorous introduction to quantitative empirical research methods, designed for doctoral students in the social sciences and master’s students in data analytics or computational social science. It covers probability, hypothesis testing, and regression analysis, with a strong focus on simple and multiple linear regression, as well as logistic regression and generalized linear models. The course also critically examines conventional statistical practices, addressing issues such as the replication crisis, p-values, and limitations of null hypothesis testing. Students will engage in simulations and data analysis using R, developing both technical proficiency and a deeper understanding of statistical methodologies.\n\nOpen Syllabus"
  },
  {
    "objectID": "Teaching.html#dacss-604-advanced-data-driven-storytelling",
    "href": "Teaching.html#dacss-604-advanced-data-driven-storytelling",
    "title": "Teaching",
    "section": "DACSS-604: Advanced Data-driven Storytelling",
    "text": "DACSS-604: Advanced Data-driven Storytelling\nThis hands-on course provides students with the knowledge and skills needed to generate strong, data-driven communication. We will emphasize graphic and visualization theory and practices, clear interpretation of data patterns, quantitative analyses, and statistical inferences, using precise and accessible language appropriate for one’s audience.\n\nOpen Syllabus\nDashboard for Class Demo\nLive Map and Animated GIF for Class Demo"
  },
  {
    "objectID": "Teaching.html#legal-393e-empirical-legal-studies",
    "href": "Teaching.html#legal-393e-empirical-legal-studies",
    "title": "Teaching",
    "section": "Legal-393E: Empirical Legal Studies",
    "text": "Legal-393E: Empirical Legal Studies\nFrom academic research on the impact of landmark court cases to the judge reading a brief loaded with social policy arguments, the study of law frequently involves empirical data. In this course, students will both learn how to digest empirical legal research and be equipped with the skills to conduct their own empirical research. Using readings and accompanying data from a variety of areas of legal studies research, the course covers topics including constructing empirical legal research questions, collecting and generating data appropriate for testing those questions, visualizing data, and conducting descriptive and inferential analyses. Students will leave the course with a firm understanding of the centrality of data in twenty-first-century law.\n\nOpen Syllabus\nQuiz Solutions and Additional Teaching Notes:\n\nTutorial 1: Quiz#1 & R Basics\nTutorial 2: Quiz#2, Reading Data & R Project\nTutorials 3 and 4: Quiz#3 and 4; Summary Statistics & User-written Functions in R.\nTutorial 5 and Quiz#5: t-test, Regression; Visualization & Interpretation\n\nGroup Project Labs:\n\n\nGroup Project Lab(1): Data Cleaning and Descriptive Statistics"
  },
  {
    "objectID": "Teaching.html#other-teaching-materials-politics-of-china",
    "href": "Teaching.html#other-teaching-materials-politics-of-china",
    "title": "Teaching",
    "section": "Other Teaching Materials: Politics of China",
    "text": "Other Teaching Materials: Politics of China"
  },
  {
    "objectID": "teaching_Examples.html",
    "href": "teaching_Examples.html",
    "title": "Example Graphs for Teaching",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(smss)\nlibrary(stargazer)\n\n\nPlease cite as: \n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(mlbench)"
  },
  {
    "objectID": "teaching_Examples.html#spurious-relationship",
    "href": "teaching_Examples.html#spurious-relationship",
    "title": "Example Graphs for Teaching",
    "section": "Spurious Relationship",
    "text": "Spurious Relationship\n\nFirst, let’s start with some simple tasks to refresh the basics of R & RStudio. 1.Check your working directory (type your code below and run it)\n\n\nlibrary(ggplot2)\n\n# Create the dataset\nset.seed(42)  # For reproducibility\ndata &lt;- data.frame(\n  Temperature = seq(60, 100, length.out = 20),  # Temperature (Z) as the true driver\n  Bottle_Water_Sales = seq(50, 200, length.out = 20) + rnorm(20, 0, 10),  # X: Water bottles sold\n  Sentencing_Length = seq(5, 15, length.out = 20) + rnorm(20, 0, 2)  # Y: Sentence length (years)\n)\n\n# Plot the spurious relationship (ignoring temperature)\nggplot(data, aes(x = Bottle_Water_Sales, y = Sentencing_Length)) +\n  geom_point(color = \"blue\", size = 3) +  # Scatter points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Fake trend line\n  labs(\n    title = \"Bottle Water Sales & Sentencing Length\",\n    x = \"Bottle Water Sold in Courtrooms\",\n    y = \"Sentencing Length (Years)\",\n    caption = \"Note: Simulated Data Enlightened by Blair, IV et.al. (2004) and Craigie, Terry-Ann et.al(2023).\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "teaching_Examples.html#collider",
    "href": "teaching_Examples.html#collider",
    "title": "Example Graphs for Teaching",
    "section": "Collider",
    "text": "Collider\n\nset.seed(123)\n\nn &lt;- 500  \n\n# Generate exercise levels (hours per week)\nexercise &lt;- rnorm(n, mean = 5, sd = 2)\n\n# Generate blood pressure - Randomly generated, NO relationship with exercise\nblood_pressure &lt;- rnorm(n, mean = 120, sd = 5)\n\n# Obesity is influenced by both exercise and blood pressure (collider)\nobesity &lt;- ifelse(exercise &lt; 5 | blood_pressure &gt; 120, 1, 0)\n\n# Modify Blood Pressure to create group-specific effects\nblood_pressure[obesity == 0] &lt;- blood_pressure[obesity == 0] - 2 * exercise[obesity == 0]  # Exercise LOWERS BP for non-obese\nblood_pressure[obesity == 1] &lt;- blood_pressure[obesity == 1] + 2 * exercise[obesity == 1]  # Exercise RAISES BP for obese\n\n# Create dataframe\ndata &lt;- data.frame(exercise, blood_pressure, obesity)\n\nModeling:\n\n# Overall regression (ignoring obesity)\noverall &lt;- lm(blood_pressure ~ exercise, data = data)\n\n# Regression within subgroup\nobese &lt;- lm(blood_pressure ~ exercise, data = data %&gt;% filter(obesity == 1))\nnon_obese &lt;- lm(blood_pressure ~ exercise, data = data %&gt;% filter(obesity == 0))\n\n# Controlling for obesity\ncontrol &lt;- lm(blood_pressure ~ exercise + obesity, data = data)\n\nstargazer(overall, control,\n          type = \"text\")\n\n\n======================================================================\n                                   Dependent variable:                \n                    --------------------------------------------------\n                                      blood_pressure                  \n                              (1)                      (2)            \n----------------------------------------------------------------------\nexercise                   -1.593***                 2.048***         \n                            (0.314)                  (0.135)          \n                                                                      \nobesity                                             32.395***         \n                                                     (0.586)          \n                                                                      \nConstant                  130.907***                88.997***         \n                            (1.705)                  (0.991)          \n                                                                      \n----------------------------------------------------------------------\nObservations                  500                      500            \nR2                           0.049                    0.867           \nAdjusted R2                  0.047                    0.866           \nResidual Std. Error    13.651 (df = 498)         5.112 (df = 497)     \nF Statistic         25.717*** (df = 1; 498) 1,618.847*** (df = 2; 497)\n======================================================================\nNote:                                      *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nPlotting\n\nlibrary(ggplot2)\n\n# Fit regression models\noverall &lt;- lm(blood_pressure ~ exercise, data = data)  \ncontrol &lt;- lm(blood_pressure ~ exercise + obesity, data = data)\n\n# Create a new dataset for predictions\nexercise_seq &lt;- seq(min(data$exercise), max(data$exercise), length.out = 100)\npred_data &lt;- data.frame(exercise = exercise_seq, obesity = 0)  # Default obesity = 0\npred_data$overall_pred &lt;- predict(overall, newdata = pred_data)  # Predictions from model without obesity\npred_data$control_pred &lt;- predict(control, newdata = pred_data)  # Predictions from model with obesity\n\n# Plot the observed data\nggplot(data, aes(x = exercise, y = blood_pressure)) +\n  \n  # Scatter plot of actual data\n  geom_point(alpha = 0.5, color = \"gray\") +\n  \n  # Regression line for overall model (ignoring obesity)\n  geom_line(data = pred_data, aes(x = exercise, y = overall_pred), color = \"black\", linetype = \"dashed\", size = 1) +\n  \n  # Regression line for controlled model (including obesity)\n  geom_line(data = pred_data, aes(x = exercise, y = control_pred), color = \"red\", size = 1) +\n  \n  # Labels and formatting\n  labs(\n    title = \"Collider Bias: Exercise & Blood Pressure\",\n    subtitle = \"Dashed Black Line = Overall Model, Red Line = Controlling for Obesity\",\n    x = \"Exercise (Hours per Week)\",\n    y = \"Blood Pressure (mmHg)\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "Legal393E_Notes/Lab2.html",
    "href": "Legal393E_Notes/Lab2.html",
    "title": "Legel 393E Spring 2025: Lab Meeting#2: Tutorial#2 Quiz Solutions; R Project; Loading Dataset & Data Structure",
    "section": "",
    "text": "This week, we talk about data collection and coding. In today’s lab, in addition to the functions in Tutorial#2, let’s explore some basic operations of loading a dataset and coding some data."
  },
  {
    "objectID": "Legal393E_Notes/Lab2.html#loading-a-dataset.",
    "href": "Legal393E_Notes/Lab2.html#loading-a-dataset.",
    "title": "Legel 393E Spring 2025: Lab Meeting#2: Tutorial#2 Quiz Solutions; R Project; Loading Dataset & Data Structure",
    "section": "Loading a dataset.",
    "text": "Loading a dataset.\nIn Tutorial#2, all the three datasets are pre-loading for you already. However, these datasets are not pre-loaded in the RStudio console on our devices. How can we load them?\nFirst, you need to go to the sources of the datasets (pseo: select “View Files” and download the .CSV file; electric: download the .CSV file) and download them to the local device manually.\nThe next step is to read these downloaded data files into R. Since data files can be stored in different formats:\n\nspreadsheets: comma-separated values (.csv file), excel and Google Sheets (.xls file);\nother statistical software (SPSS, STATA, SAS);\ntext files (.txt);\ndatabase (.sql);\nother unstructured data: image, text, audio, video;\nYou can even ask R to read a data files online.\n\nWe don’t usually encounter the last two types of data files in legal studies, and they require specific R packages to be read. With the first two types of data, we primarily use the following functions with the two pre-installed packages to read.\n\nlibrary(readxl) #for spreadsheets\nlibrary(readr) #also for spreadsheets\nlibrary(haven) #for SPSS, STATA, and SAS\n\nIf we know what packges we need to use for a project, we can load them all at once:\n\n#create a vector contains the names of the needed packages, and assign a name \"needed.packages\" for it.\nneeded.packages&lt;-c('readxl','readr','haven','dplyr')\n\n#lapply() allows us to loop over and execute every element in the object \"needed.packages\".\nlapply(needed.packages, require, character.only = TRUE)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n\nFor .txt files, we have multiple function options to use. For example:\nread.table(): General function for reading tabular data from a text file.\nread.delim(): Specialized function for tab-separated files (.txt with tab separation).\nBefore I load the two datasets into R, there are two more things I want to introduce.\n\nFirst, check your data path and working directory.\nWhen loading a dataset in R, you need to specify the file path correctly. There are two ways to do this:\n\n(1) Using an Absolute Path\nIf your dataset is stored outside the working mustdirectory, you must provide the full file path (absolute path) to load it.\n\n\nExample: Absolute Path (.csv file)\nIn windows:\ndata&lt;-read.csv(\"C:/Users/YourName/Documents/data.csv\")\nOn macOS/Linux:\ndata &lt;- read.csv(\"/Users/YourName/Documents/data.csv\")\nUsing the absolute path, you don’t have to worry about the current working directory. However, the path is specific to your computer and will not work if the R script is shared with someone else. In addition, it may not work if the file is moved to a different location.\n\n\n\n(2) Using a Relative Path\nIf your dataset is stored inside the working directory, you can simply use the relative path.\n\nExample: Relative Path:\ndata &lt;- read.csv(“data.csv”)\n\n\nor if it’s inside a subfolder:\ndata &lt;- read.csv(“data/my_data.csv”)\nPersonally, I prefer to the second method since I generally work on my data in different R projects (see the introduction of R Project after the Quiz Solutions), my datasets are always stored within the working directory.\n\n\n\nSecond, preview the dataset before loading it, see if it needs to be adjusted.\nIf the data file is a spreadsheet, open it in Excel to preview its structure because many of the original data spreadsheets are not “tidy data”(each row is a case of observation, and each column represents a specific variable), which we can be directly load and use it.use it.\nNow I am opening the data file for electricity (file name: API_EG.ELC.ACCS.ZS_DS2_en_csv_v2_76137.csv). Wow, the data looks very different from the one we loaded in tutorial#2: there are headers, titles, notes, and some columns/variables that are not in the tutorial (Indicator Name, Indicator Code). It also includes columns from 1960 to 2023.\n\nBut I want to read just the data of the percentage of the population with access to electricity from every country between 1990 and 2016. How can we make it look like the data we load in tutorial#2? Here, we need to do some data transformation and tidying, including using some of the functions included in tutorial#2. You can also refer to the textbook I suggested.\n\nlibrary(readr)\nelectric &lt;- read_csv(\"data/API_EG.ELC.ACCS.ZS_DS2_en_csv_v2_76137.csv\", \n    skip = 3)\n\nNew names:\nRows: 266 Columns: 69\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): Country Name, Country Code, Indicator Name, Indicator Code dbl (33): 1990,\n1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, ... lgl (32): 1960,\n1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...69`\n\nhead(electric) #preview the dataset and the types of each column\n\n\n  \n\n\n\n\n#using dplyr packages (a tidyverse family package)to help to edit and change dataset. Since we have loaded the package above, we don't have to load it again.\n\nelectric &lt;- electric %&gt;% \n  select(\"Country Name\", \"Country Code\", as.character(1990:2016)) # using select() for us to keep only the columns we want.\n\n\n\n#\"|&gt;\" or \"%&gt;%\" is the pipe operator. It is used to pass the output of one function directly as the input to the next, making code more readable and reducing the need for nested function calls.\n\n\n#Preview the data again\nhead(electric)\n\n\n  \n\n\n#Show the dataset dimension\ndim(electric)\n\n[1] 266  29\n\n\nUse dim() , we can see the dataset contains 266 countries (rows) and 29 columns. It has more countries (rows) than the one preloaded in tutorial#2 because this dataset has been updated since tutorial#2 wa made.\nSince we will be using the pseo dataset in the tutorial#2, let me load it here.\n\npseo &lt;- read_csv(\"https://raw.githubusercontent.com/RosemaryPang/Data-for-teaching/main/pseo_601.csv\")\n\nRows: 18783 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): institution_name, deglevl, ciptitle, grad_cohort_label, state\ndbl (9): institution_id, deglevl_code, degcip_4dig, grad_cohort, year_postgr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(pseo)\n\n\n  \n\n\ndim(pseo)\n\n[1] 18783    14"
  },
  {
    "objectID": "Legal393E_Notes/Lab2.html#tutorial2-quiz-answers",
    "href": "Legal393E_Notes/Lab2.html#tutorial2-quiz-answers",
    "title": "Legel 393E Spring 2025: Lab Meeting#2: Tutorial#2 Quiz Solutions; R Project; Loading Dataset & Data Structure",
    "section": "Tutorial#2 Quiz Answers",
    "text": "Tutorial#2 Quiz Answers\n\nQuestion 1. What percentage of Afghanistan had access to electricity in 1990, according to the electric dataset?\nAnswer: we will use select() to get Afghanistan and the column of 1990.\n\n select(electric, \"Country Name\", \"1990\")\n\n\n  \n\n\n\nA more straightfoward way to see a particualr country is to use filter() to get the particular row.\n\nafghanistan_data &lt;- electric %&gt;% \n  select(\"Country Name\", \"1990\") %&gt;%\n  filter(`Country Name` == \"Afghanistan\")\n\nafghanistan_data\n\n\n  \n\n\n\nWell, since the dataset I load here is not entirely the same as the one in tutorial#2, the results is different. But you should get the correct answer using these codes.\n\n\nQuestion 2. How many rows are in the pseo dataset?\nAnswer:\n\n#Two options:\n#1.Using dim()\n\ndim(pseo)\n\n[1] 18783    14\n\n#2.nrow()\n\nnrow(pseo)\n\n[1] 18783\n\n\n\n\nQuestion 3. How many observations are in the pseo dataset for the University of Colorado Denver?\nAnswer: According to Tutotiral#2, we can use table() to get the anawer.\n\nschool&lt;-select(pseo,institution_name)\ntable(school)\n\ninstitution_name\n                     ADAMS STATE COLLEGE \n                                     208 \n                  AIMS COMMUNITY COLLEGE \n                                     365 \n              ARAPAHOE COMMUNITY COLLEGE \n                                     452 \n                COLORADO MESA UNIVERSITY \n                                     418 \n               COLORADO MOUNTAIN COLLEGE \n                                     330 \n COLORADO NORTHWESTERN COMMUNITY COLLEGE \n                                     242 \n                COLORADO SCHOOL OF MINES \n                                     174 \n               COLORADO STATE UNIVERSITY \n                                     885 \n        COLORADO STATE UNIVERSITY-PUEBLO \n                                     331 \nCOLORADO STATE UNIVERSITY - GLOBAL CAMPU \n                                      28 \n             COMMUNITY COLLEGE OF AURORA \n                                     267 \n             COMMUNITY COLLEGE OF DENVER \n                                     393 \n        EMILY GRIFFITH TECHNICAL COLLEGE \n                                     140 \n                      FORT LEWIS COLLEGE \n                                     280 \n           FRONT RANGE COMMUNITY COLLEGE \n                                     449 \n                 LAMAR COMMUNITY COLLEGE \n                                     126 \n METROPOLITAN STATE UNIVERSITY OF DENVER \n                                     514 \n                MORGAN COMMUNITY COLLEGE \n                                     223 \n             NORTHEASTERN JUNIOR COLLEGE \n                                     195 \n                    OTERO JUNIOR COLLEGE \n                                     190 \n               PICKENS TECHNICAL COLLEGE \n                                     144 \n            PIKES PEAK COMMUNITY COLLEGE \n                                     523 \n                PUEBLO COMMUNITY COLLEGE \n                                     416 \n             RED ROCKS COMMUNITY COLLEGE \n                                     516 \n               TRINIDAD STATE JR COLLEGE \n                                     348 \n        UNIV OF COLORADO AT COLO SPRINGS \n                                     370 \n     UNIV OF TEXAS HEALTH SCIENCE CENTER \n                                      82 \n            UNIV OF TEXAS MEDICAL BRANCH \n                                      80 \n      UNIV OF TEXAS SOUTHWESTERN MED CTR \n                                      88 \n       UNIV OF TX MD ANDERSON CANCER CTR \n                                      30 \n          UNIVERSITY OF COLORADO-BOULDER \n                                     770 \n           UNIVERSITY OF COLORADO DENVER \n                                     519 \n      UNIVERSITY OF MICHIGAN - ANN ARBOR \n                                    1168 \n         UNIVERSITY OF NORTHERN COLORADO \n                                     528 \n           UNIVERSITY OF TEXAS-ARLINGTON \n                                     720 \n              UNIVERSITY OF TEXAS-AUSTIN \n                                    1025 \n             UNIVERSITY OF TEXAS-EL PASO \n                                     667 \n       UNIVERSITY OF TEXAS-PERMIAN BASIN \n                                     337 \n         UNIVERSITY OF TEXAS-SAN ANTONIO \n                                     690 \n       UNIVERSITY OF TEXAS - BROWNSVILLE \n                                     351 \n      UNIVERSITY OF TEXAS - PAN AMERICAN \n                                     578 \n           UNIVERSITY OF TEXAS AT DALLAS \n                                     490 \n            UNIVERSITY OF TEXAS AT TYLER \n                                     428 \nUNIVERSITY OF TEXAS HEALTH SCIENCE CENTE \n                                       1 \n         UNIVERSITY OF WISCONSIN-MADISON \n                                    1353 \n            UT HEALTH CENTER-SAN ANTONIO \n                                     118 \n       WESTERN STATE COLORADO UNIVERSITY \n                                     233 \n\n\nWe can also use the filter() function to keep only the rows of the school, so you don’t have to browse the table and find the number yourself.\n\npseo %&gt;%\n  filter(institution_name == \"UNIVERSITY OF COLORADO DENVER\") %&gt;%\n  count()\n\n\n  \n\n\n\n\n\nQuestion 4. Which of the following functions could be used on a dataset to return the names of all the variables in the dataset?\nAnswer: all the variables in a “tidy” data in R are listed as columns. So the answer is colnames() .\n\n\nQuestion 5. Imagine I had a class roster stored in R that included a variable called “year” that featured the year of entry for each student. If I wanted to how many students fell into each year (i.e., a count of how many students entered in each year), which of the following would return a table with those values?\nAnswer: like Question 3, we use the table function: table(year) ."
  },
  {
    "objectID": "Legal393E_Notes/Lab2.html#additional-introduction-to-r-basic-operations",
    "href": "Legal393E_Notes/Lab2.html#additional-introduction-to-r-basic-operations",
    "title": "Legel 393E Spring 2025: Lab Meeting#2: Tutorial#2 Quiz Solutions; R Project; Loading Dataset & Data Structure",
    "section": "Additional Introduction to R Basic Operations",
    "text": "Additional Introduction to R Basic Operations\nAlthough this is not an R coding course, I suggest you do more research to learn R, including reading this suggested textbook. Besides providing you with the tutorials and the lab days, I will also provide some additional notes on R coding practices.\nThis week, I want to introduce you to the following things: R project, Data Recoding, and Data Editing.\n\nIntroduction to R Projects: why you need to create one when working on a project.\nWhen working with R, it’s crucial to maintain an organized workflow, especially as your project contains multiple files (datasets, r scripts, graphs, etc.) and the analyses become more complex. One of the best ways to do this is using R Projects in RStudio.\n\nWhat is an R Project?\nAn R Project is a self-contained working environment managed by RStudio. It helps you keep all related files (scripts, data, plots, and outputs) in one place, making it easier to manage your work. When you open an R Project, R automatically sets the working directory to the project folder, ensuring consistency in file paths and data loading.\n\n\nWhy Use an R Project?\n\nOrganized Workflow – Keeps all files related to a project in a single directory, preventing confusion and lost files.\nConsistent Working Directory – Automatically sets the working directory when you open the project, eliminating the need to use setwd(), which can cause problems across different computers.\nReproducibility – Helps ensure your scripts work reliably without hard-coded paths that break when shared with others.\nSeamless Collaboration – Makes it easier to share your work with others (e.g., through GitHub) since all necessary files are contained within the project.\nIntegrated with Version Control – Works well with Git and GitHub, allowing for easy version tracking and collaboration.\n\n\n\nHow to Create an R Project in RStudio\n\nIn RStudio, go to File &gt; New Project.\nChoose one of the following options:\n\nNew Directory: Create a new folder for your project.\nExisting Directory: Turn an existing folder into an R Project.\nVersion Control: Clone a project from GitHub. (You don’t need to learn this now)\n\nName your project and select a location to save it.\nClick Create Project.\n\nOnce the project is created, you’ll see a .Rproj file inside the project folder. RStudio uses this file to manage your project environment.\n\n\n\nBest Practices When Using R Projects\n\nAlways use relative paths when referencing files (e.g., \"data/mydata.csv\") instead of absolute paths (\"C:/Users/.../mydata.csv\").\nKeep your scripts, data, and outputs organized in separate subfolders within your project.\nUse version control (Git/GitHub) to track changes and collaborate with others effectively.\nRegularly document your work using a README.md file or R Markdown."
  },
  {
    "objectID": "Legal393E_Notes/Lab2.html#data-structure-and-object-types",
    "href": "Legal393E_Notes/Lab2.html#data-structure-and-object-types",
    "title": "Legel 393E Spring 2025: Lab Meeting#2: Tutorial#2 Quiz Solutions; R Project; Loading Dataset & Data Structure",
    "section": "Data Structure and Object Types",
    "text": "Data Structure and Object Types\n#Object Types 1. Please create the four different types of atomic vectors. Make sure they at least have 4 elements (length = 4). You can double-check their types by function typeof()\n\nlogical &lt;- c(TRUE, FALSE, FALSE)\n\ninteger&lt;-c(3, 10)\n\ndouble &lt;- c(1.5, 2.8, pi)\n\ncharacter &lt;- c(\"this\", \"is\", \"a character\", \"vector\")\n\nHmm, you may find the R classified the “integer vector” you created as “double”. This is because a number by default is considered double in R. So can we make this vector as integer? Two options:(1)adding “L” after each number you enter; and (2)coersion.\n\n#option 1:\nnew_integer&lt;-c(3L, 10L)\ntypeof(new_integer)\n\n[1] \"integer\"\n\n#option 2:\ncoercion_integer&lt;-as.integer(integer)\ntypeof(coercion_integer)\n\n[1] \"integer\"\n\n\n\nNow, try to extract the following things from each of the above vectors: (1) the first element; (2)the second to third element; (3) the last element\n\n\n#the first element\ndouble[1]\n\n[1] 1.5\n\n#the second to third element\ndouble[2:3]\n\n[1] 2.800000 3.141593\n\n#the last element\ntail(double, 1)\n\n[1] 3.141593\n\n\n\nLet’s create a list combing the above four atomic vectors.\n\n\nmy_list&lt;-list(integer, double, character, logical)\nmy_list\n\n[[1]]\n[1]  3 10\n\n[[2]]\n[1] 1.500000 2.800000 3.141593\n\n[[3]]\n[1] \"this\"        \"is\"          \"a character\" \"vector\"     \n\n[[4]]\n[1]  TRUE FALSE FALSE\n\n\n\nNext, create a simple n by n tibble of your own. You can use the previous vectors you created above.\n\n\nmy_tibble&lt;-tibble(logical, double)\nmy_tibble\n\n\n  \n\n\n\nAnd this is the very first dataframe we create!"
  },
  {
    "objectID": "Legal393E_Notes/Lab2.html#tutorial2-quiz-answers-will-be-released-lateryou-can-send-me-an-email-for-the-answer-keys-if-you-have-completed-quiz-2.",
    "href": "Legal393E_Notes/Lab2.html#tutorial2-quiz-answers-will-be-released-lateryou-can-send-me-an-email-for-the-answer-keys-if-you-have-completed-quiz-2.",
    "title": "Legel 393E Spring 2025: Lab Meeting#2: Tutorial#2 Quiz Solutions; R Project; Loading Dataset & Data Structure",
    "section": "Tutorial#2 Quiz Answers (Will be released later;you can send me an email for the answer keys if you have completed Quiz 2.)",
    "text": "Tutorial#2 Quiz Answers (Will be released later;you can send me an email for the answer keys if you have completed Quiz 2.)\n\nQuestion 1. What percentage of Afghanistan had access to electricity in 1990, according to the electric dataset?\nAnswer: we will use select() to get Afghanistan and the column of 1990.\n\n# select(electric, \"Country Name\", \"1990\")\n\nA more straightfoward way to see a particualr country is to use filter() to get the particular row.\n\n#afghanistan_data &lt;- electric %&gt;% \n#  select(\"Country Name\", \"1990\") %&gt;%\n#  filter(`Country Name` == \"Afghanistan\")\n\n#afghanistan_data\n\nWell, since the dataset I load here is not entirely the same as the one in tutorial#2, the results is different. But you should get the correct answer using these codes.\n\n\nQuestion 2. How many rows are in the pseo dataset?\nAnswer:\n\n#Two options:\n#1.Using dim()\n\n#dim(pseo)\n\n#2.nrow()\n\n#nrow(pseo)\n\n\n\nQuestion 3. How many observations are in the pseo dataset for the University of Colorado Denver?\nAnswer: Tutorial#2’s codes:\n\n#school&lt;-select(pseo,institution_name)\n#table(school)\n\nWe can also use the filter() function to keep only the rows of the school, so you don’t have to browse the table and find the number yourself.\n\n#pseo %&gt;%\n#  filter(institution_name == \"UNIVERSITY OF #COLORADO DENVER\") %&gt;%\n#  count()\n\n\n\nQuestion 4. Which of the following functions could be used on a dataset to return the names of all the variables in the dataset?\nAnswer: all the variables in a “tidy” data in R are listed as columns. So the answer is colnames() .\n\n\nQuestion 5. Imagine I had a class roster stored in R that included a variable called “year” that featured the year of entry for each student. If I wanted to how many students fell into each year (i.e., a count of how many students entered in each year), which of the following would return a table with those values?\nAnswer: like Question 3, we use the table function: table(year) ."
  },
  {
    "objectID": "Legal393E_Notes/Lab2.html#additional-introduction-to-r-basic-operations-r-project",
    "href": "Legal393E_Notes/Lab2.html#additional-introduction-to-r-basic-operations-r-project",
    "title": "Legel 393E Spring 2025: Lab Meeting#2: Tutorial#2 Quiz Solutions; R Project; Loading Dataset & Data Structure",
    "section": "Additional Introduction to R Basic Operations: R Project",
    "text": "Additional Introduction to R Basic Operations: R Project\nAlthough this is not an R coding course, I suggest you do more research to learn R, including reading this suggested textbook. Besides providing you with the tutorials and the lab days, I will also provide some additional notes on R coding practices.\nThis week, I want to introduce you to the following things: R project, Data Recoding, and Data Editing.\n\nIntroduction to R Projects: why you need to create one when working on a project.\nWhen working with R, it’s crucial to maintain an organized workflow, especially as your project contains multiple files (datasets, r scripts, graphs, etc.) and the analyses become more complex. One of the best ways to do this is using R Projects in RStudio.\n\nWhat is an R Project?\nAn R Project is a self-contained working environment managed by RStudio. It helps you keep all related files (scripts, data, plots, and outputs) in one place, making it easier to manage your work. When you open an R Project, R automatically sets the working directory to the project folder, ensuring consistency in file paths and data loading.\n\n\nWhy Use an R Project?\n\nOrganized Workflow – Keeps all files related to a project in a single directory, preventing confusion and lost files.\nConsistent Working Directory – Automatically sets the working directory when you open the project, eliminating the need to use setwd(), which can cause problems across different computers.\nReproducibility – Helps ensure your scripts work reliably without hard-coded paths that break when shared with others.\nSeamless Collaboration – Makes it easier to share your work with others (e.g., through GitHub) since all necessary files are contained within the project.\nIntegrated with Version Control – Works well with Git and GitHub, allowing for easy version tracking and collaboration.\n\n\n\nHow to Create an R Project in RStudio\n\nIn RStudio, go to File &gt; New Project.\nChoose one of the following options:\n\nNew Directory: Create a new folder for your project.\nExisting Directory: Turn an existing folder into an R Project.\nVersion Control: Clone a project from GitHub. (You don’t need to learn this now)\n\nName your project and select a location to save it.\nClick Create Project.\n\nOnce the project is created, you’ll see a .Rproj file inside the project folder. RStudio uses this file to manage your project environment.\n\n\n\nBest Practices When Using R Projects\n\nAlways use relative paths when referencing files (e.g., \"data/mydata.csv\") instead of absolute paths (\"C:/Users/.../mydata.csv\").\nKeep your scripts, data, and outputs organized in separate subfolders within your project.\nUse version control (Git/GitHub) to track changes and collaborate with others effectively.\nRegularly document your work using a README.md file or R Markdown."
  },
  {
    "objectID": "Legal393E_Notes/Lab3.html",
    "href": "Legal393E_Notes/Lab3.html",
    "title": "Legel 393E Spring 2025: Lab Meeting#3: Tutorial#3-4 Quiz Solutions",
    "section": "",
    "text": "This week, we will go over the solutions for Quiz 3, 4 and 5. In addition, I will give some additional notes on how to explain regression results.\nNext time, we will discuss data cleaning and other additional operations in R."
  },
  {
    "objectID": "Legal393E_Notes/Lab3.html#loading-the-necessary-datasets.",
    "href": "Legal393E_Notes/Lab3.html#loading-the-necessary-datasets.",
    "title": "Legel 393E Spring 2025: Lab Meeting#3: Tutorial#3-4 Quiz Solutions",
    "section": "Loading the necessary datasets.",
    "text": "Loading the necessary datasets.\n\n#create a vector contains the names of the needed packages, and assign a name \"needed.packages\" for it.\nneeded.packages&lt;-c('readxl','readr','haven','dplyr','ggplot2')\n\n#lapply() allows us to loop over and execute every element in the object \"needed.packages\".\nlapply(needed.packages, require, character.only = TRUE)\n\nLoading required package: readxl\n\n\nLoading required package: readr\n\n\nLoading required package: haven\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n\n\nElectricity data\n\n#Check working directory\ngetwd()\n\n[1] \"C:/Users/donge/OneDrive/Desktop/GitHub/webpage/Legal393E_Notes\"\n\nelectric &lt;- read_csv(\"data/API_EG.ELC.ACCS.ZS_DS2_en_csv_v2_76137.csv\", \n    skip = 3)\n\nNew names:\nRows: 266 Columns: 69\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): Country Name, Country Code, Indicator Name, Indicator Code dbl (33): 1990,\n1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, ... lgl (32): 1960,\n1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...69`\n\nhead(electric) #preview the dataset and the types of each column\n\n\n  \n\n\n\n\n#using dplyr packages (a tidyverse family package)to help to edit and change dataset. Since we have loaded the package above, we don't have to load it again.\n\nelectric &lt;- electric %&gt;% \n  select(\"Country Name\", \"Country Code\", as.character(1990:2016)) # using select() for us to keep only the columns we want.\n\n\n#Preview the data again\nhead(electric)\n\n\n  \n\n\n#Show the dataset dimension\ndim(electric)\n\n[1] 266  29\n\n\n\n\nLoading pseo data\n\npseo &lt;- read_csv(\"https://raw.githubusercontent.com/RosemaryPang/Data-for-teaching/main/pseo_601.csv\")\n\nRows: 18783 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): institution_name, deglevl, ciptitle, grad_cohort_label, state\ndbl (9): institution_id, deglevl_code, degcip_4dig, grad_cohort, year_postgr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(pseo)\n\n\n  \n\n\ndim(pseo)\n\n[1] 18783    14\n\n\n\n\nLoading anes data\n\nanes&lt;- read_csv(\"https://raw.githubusercontent.com/RosemaryPang/Data-for-teaching/main/anes_601.csv\")   \n\nRows: 4271 Columns: 440\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (440): CASEID, v162002, v162003, v162004, v162005, v162006, v162007, v16...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(anes)   \n\n\n  \n\n\ndim(anes)\n\n[1] 4271  440\n\n#recode v162136x as Econ_Mobility, v162034a as Vote_16 and v162132 as Class.\n\nanes&lt;-anes%&gt;%\n  mutate(Econ_Mobility = case_when(\n         v162136x == 1 ~ \"Much Easier\",\n         v162136x == 2 ~ \"Moderately Easier\",\n         v162136x == 3 ~ \"Slightly Easier\",\n         v162136x == 4 ~ \"The Same\",\n         v162136x == 5 ~ \"Slightly Harder\",\n         v162136x == 6 ~ \"Moderately Harder\",\n         v162136x == 7 ~ \"Much Harder\"\n         )) %&gt;%\n  mutate(Vote_16 = case_when(\n         v162034a == 1 ~ \"Clinton\",\n         v162034a == 2 ~ \"Trump\",\n         v162034a == 3 ~ \"Johnson\",\n         v162034a == 4 ~ \"Stein\"\n         )) %&gt;%\n  mutate(Class = case_when(\n    v162132 == 1 ~ \"Lower Class\",\n    v162132 == 2 ~ \"Working Class\",\n    v162132 == 3 ~ \"Middle Class\",\n    v162132 == 4 ~ \"Upper Class\"\n  ))"
  },
  {
    "objectID": "Legal393E_Notes/Lab3.html#tutorial3-quiz-answers",
    "href": "Legal393E_Notes/Lab3.html#tutorial3-quiz-answers",
    "title": "Legel 393E Spring 2025: Lab Meeting#3: Tutorial#3-4 Quiz Solutions",
    "section": "Tutorial#3 Quiz Answers",
    "text": "Tutorial#3 Quiz Answers\n\nQuestion 1. In the ANES data, among which class of voters did Gary Johnson earn 19 votes in the 2016 presidential election?\nAnswer: we can answer this question by creating a crosstabs of social class and voter choice.\n\n#Make crosstabs for Vote_16 and Econ_Mobility\nxtabs(~ Class + Vote_16, anes)\n\n               Vote_16\nClass           Clinton Johnson Stein Trump\n  Lower Class        20       3     3    21\n  Middle Class      267      19     3   247\n  Upper Class        28       1     0    10\n  Working Class     159      15     5   166\n\n\nSo the answer is Middle Class.\n\n\nQuestion 2. In ggplot2, you can add multiple layers and options to your plots.\nAnswer: Yes. As we know, ggplot2() provides a unifying framework (a grammar) to program visualizations. Starting with the basic ggplot() , we pass our dataset and create an empty plot object. From there, we use aes() within ggplot() to specify the model and variables. Next, we add (+) geometry layers to specify the graphic items and graphic types. Finally, we add other layers to customize the graph notations.\nFor example, let me introduce how to make a stacked-to-100% barplot. Note the argument in geom_bar() function: position=\"fill\" tells R that the percentage of each subgroup is represented within a bar, allowing to study the evolution of their proportion in the whole.\n\n#Plotting class and vote choice of ANES data\nggplot(filter(anes, !is.na(Class)), aes(fill=Class,x = Vote_16))+\n  #!is.na() remove N/A or missing data in a variable;\n  #make sure \"+\" is following the ggplot() and the layer function!\n  #The fill color of the bars will be determined by the Class variable.\n geom_bar(position=\"fill\")  +\n labs(title = \"Class and Voting\", y = \"2016 American National Election Study\", x = \"Voting for Presidential Candidates\")\n\n\n\n\n\n\n\n\n\n\nQuestion 3. In which of the following years do the most countries have no or close to zero access to electricity?\nAnswer:\nNote: the electric data used in this demonstration is not the same as the one in Tutorial#3. So the results will be different. The correct answer is 1990.\nThe simpliest way is to plot the three years (1990, 2003, and 2016) and compare. For example, 1990:\n#Make sure you exclude NA before plotting. You can do it in the ggplot() function using !is.na() like below, or you can filter out all NAs before plotting like this electric_clean &lt;- electric_long %&gt;% filter(!is.na(access))\n\nggplot(electric %&gt;% filter(!is.na(`1990`)), \n       aes(x = reorder(`Country Name`, `1990`), y = `1990`)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Electricity Access by Country in 1990\",\n       x = \"Country\",\n       y = \"Access to Electricity (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe graph definitely look messy since there are so many countries and they stack over each others. Solutions?\nWe can actually plot a histogram! Because a histogram allows us to define the binwidth to customize the size or the range of values represnted by each bar, we can group countries.\n\nggplot(electric %&gt;% filter(!is.na(`1990`)), aes(x = `1990`)) +\n  geom_histogram(binwidth = 10, fill = \"red\", color = \"white\") +\n  labs(title = \"Distribution of Electricity Access in 1990\",\n       x = \"Access to Electricity (%)\",\n       y = \"Number of Countries\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAnd we can actually present three graphs on the same panel to compare. Let me introduce one way of doing this: grid.arrange().\n\np1990&lt;-ggplot(electric %&gt;% filter(!is.na(`1990`)), aes(x = `1990`)) +\n  geom_histogram(binwidth = 10, fill = \"red\", color = \"white\") +\n  labs(title = \"Distribution of Electricity Access in 1990\",\n       x = \"Access to Electricity (%)\",\n       y = \"Number of Countries\") +\n  theme_minimal()\n\np2003&lt;-ggplot(electric %&gt;% filter(!is.na(`2003`)), aes(x = `2003`)) +\n  geom_histogram(binwidth = 10, fill = \"red\", color = \"white\") +\n  labs(title = \"Distribution of Electricity Access in 2003\",\n       x = \"Access to Electricity (%)\",\n       y = \"Number of Countries\") +\n  theme_minimal()\n\np2016&lt;-ggplot(electric %&gt;% filter(!is.na(`2016`)), aes(x = `2016`)) +\n  geom_histogram(binwidth = 10, fill = \"red\", color = \"white\") +\n  labs(title = \"Distribution of Electricity Access in 2016\",\n       x = \"Access to Electricity (%)\",\n       y = \"Number of Countries\") +\n  theme_minimal()\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ngrid.arrange(p1990, p2003, p2016, ncol = 2) #column layout; alternatively, we can use nrow = 2 for horizontal layout; we can specify how many numbers of graph to be presented on the same row or column.\n\n\n\n\n\n\n\n\n\n\nQuestion 4. How would you add the “minimal” theme to a ggplot figure?\nAnswer: simply just +theme_minimal()\n\n\nQuestion 5. You can add a smoothed density curve to your ggplot histogram object by adding ” + geom_density()”\nAnswer: the answer is yes. As we see in the tutorial example:\n\nggplot(pseo, aes(p50_earnings)) + \n  geom_histogram(aes(y = ..density..), alpha = 0.5) +\n  geom_density(alpha = 0.2, fill=\"red\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 8811 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 8811 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nHowever, if we want to do a smooth curve, or a best-fitting line, for scatterplots, we need to use geom_smooth(). For example\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") + # fits a linear model (straight line), or \n  #geom_smooth() + # Without assuming linearity; LOESS fits local curve; nonlinear and flexible;\n  labs(title = \"Fuel Efficiency vs. Car Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles per Gallon (MPG)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Legal393E_Notes/Lab3.html#tutorial4-quiz-answers",
    "href": "Legal393E_Notes/Lab3.html#tutorial4-quiz-answers",
    "title": "Legel 393E Spring 2025: Lab Meeting#3: Tutorial#3-4 Quiz Solutions",
    "section": "Tutorial#4 Quiz Answers:",
    "text": "Tutorial#4 Quiz Answers:\n\nQuestion 1. What is the mean value of access to electricity in 2015? \nAnswer: This one is pretty straightforward. But don’t forget to remove missing data before doing any statistics.\n\nmean(electric$`2015`, na.rm =T)\n\n[1] 82.73636\n\n\n\n\nQuestion 2. How could you calculate the mean of a vector containing missing values?\nAnswer: summarize(data, new.variable = mean(old.variable, na.rm = TRUE))\n\n\nQuestion 3. What’s the median of p50_earnings in the pseo dataset?\nAnswer:\n\nsummarize(pseo, median(p50_earnings, na.rm = TRUE))\n\n\n  \n\n\n#or just\nmedian(pseo$p50_earnings, na.rm = T)\n\n[1] 46790\n\n\n\n\nQuestion 4. The function summarise_all()\nAnswer: Provides a summary statistic (or statistics) for each variable in a dataframe.\n\n\nQuestion 4. You can calculate the standard deviation of a variable using the function sd()\nAnswer: True."
  },
  {
    "objectID": "Legal393E_Notes/Lab3.html#additional-introduction-to-r-basic-operations",
    "href": "Legal393E_Notes/Lab3.html#additional-introduction-to-r-basic-operations",
    "title": "Legel 393E Spring 2025: Lab Meeting#3: Tutorial#3-4 Quiz Solutions",
    "section": "Additional Introduction to R Basic Operations",
    "text": "Additional Introduction to R Basic Operations\n\n1. Other useful summary statistics tools\n(1)The R built-in summary() function.\nThis is a one-stop function allows you to get the most common summary and descriptive statistics of your data. For example:\n\nsummary(pseo)\n\n institution_id  institution_name    deglevl_code      deglevl         \n Min.   : 1345   Length:18783       Min.   : 1.000   Length:18783      \n 1st Qu.: 1368   Class :character   1st Qu.: 3.000   Class :character  \n Median : 3659   Mode  :character   Median : 5.000   Mode  :character  \n Mean   : 5500                      Mean   : 5.163                     \n 3rd Qu.: 8896                      3rd Qu.: 5.000                     \n Max.   :42439                      Max.   :18.000                     \n                                                                       \n  degcip_4dig     ciptitle          grad_cohort   grad_cohort_label \n Min.   : 100   Length:18783       Min.   :1998   Length:18783      \n 1st Qu.:1506   Class :character   1st Qu.:2001   Class :character  \n Median :4000   Mode  :character   Median :2004   Mode  :character  \n Mean   :3293                      Mean   :2005                     \n 3rd Qu.:5009                      3rd Qu.:2007                     \n Max.   :5401                      Max.   :2013                     \n                                                                    \n year_postgrad     p25_earnings     p50_earnings     p75_earnings   \n Min.   : 1.000   Min.   : 12961   Min.   : 15922   Min.   : 19087  \n 1st Qu.: 1.000   1st Qu.: 25194   1st Qu.: 35416   1st Qu.: 46790  \n Median : 1.000   Median : 34543   Median : 46790   Median : 60162  \n Mean   : 3.871   Mean   : 37864   Mean   : 51089   Mean   : 67776  \n 3rd Qu.: 5.000   3rd Qu.: 45700   3rd Qu.: 60396   3rd Qu.: 79278  \n Max.   :10.000   Max.   :201545   Max.   :319269   Max.   :466933  \n                  NA's   :8811     NA's   :8811     NA's   :8811    \n   cellcount         state          \n Min.   :  30.0   Length:18783      \n 1st Qu.:  52.0   Class :character  \n Median :  92.0   Mode  :character  \n Mean   : 181.9                     \n 3rd Qu.: 201.0                     \n Max.   :3783.0                     \n NA's   :8811                       \n\n\nor I can just specify one variable in the data:\n\nsummary(pseo$p25_earnings)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  12961   25194   34543   37864   45700  201545    8811 \n\n\n(2) Package summarytools\nThis package provides a coherent set of functions centered on data exploration and simple reporting. You can check out the user mannual here.\nThe most commonly used function is: dfSummary(). It provides type-specific information for all variables: univariate statistics and/or frequency distributions, bar charts or histograms, as well as missing data counts and proportions. Very useful to quickly, detect anomalies and identify trends at a glance.\nFor example:\n\nlibrary(summarytools)\n\nWarning: package 'summarytools' was built under R version 4.4.2\n\nview(dfSummary(pseo$p25_earnings))\n\npseo$p25_earnings was converted to a data frame\n\n\nSwitching method to 'browser'\n\n\nOutput file written: C:\\Users\\donge\\AppData\\Local\\Temp\\RtmpqAK9uG\\file5c04185b692e.html\n\n\n\n\n2. Calculating Mode (most frequent value):\nAs we mentioned in the lecture on introducing summary functions, R does not have a built-in function for mode. To calculate or to get it from a data, we will have to write out own function.\nA function is a block of code that only runs when it is called. In R, many functions are already built-in and ready to call and run. But you can also write your own using the function() keyword.\nHow to Write a Function?\nHere is the basic structure:\n\nmy_function &lt;- function(argument1, argument2, ...) {\n  # Code that does something with the inputs\n  result &lt;- do_something_with(argument1, argument1)\n  return(result)  # Optional, but good practice\n}\n\nIn this above example:\nmy_function &lt;- function(argument1, argument2, ...)\n\nmy_function is the name you are assigning to the function. You can call it anything you like (e.g., calculate_total, greet_user, etc.).\n&lt;- function(...) tells R: “I’m defining a new function.”\nInside the parentheses (...) are the arguments (inputs) that the function will take.\n\nargument1 and argument2 are examples of named inputs.\n... is optional and means “allow extra arguments.” You don’t always need it, but it’s useful if your function might take flexible inputs (like plot() or mean(..., na.rm = TRUE)).\n\n\nNext:\nresult &lt;- do_something_with(``argument1``,``argument2``)\n\nInside the function, you usually want to process the inputs somehow.\n\n\n\nThis line creates a new variable result by calling a fake function called do_something_with() on argument1 and argument2.\nIn real life, you’d replace this with something with the existing built-in functions like result &lt;- argument1 + argument2 or mean(c(argument1, argument2)).\n\nFinally:\nreturn(result)\n\ntells R what value to give back when the function is used. This is optional in R: if you omit it, R will return the last evaluated line — but using return() makes your code clearer, especially for beginners.\n\nSo here is an example of customize function to get mode from a string of values:\n\nget_mode &lt;- function(x) {\n  x &lt;- na.omit(x)  # Remove NA values\n  uniq_x &lt;- unique(x) #This gets all the unique values in x\n  freq &lt;- tabulate(match(x, uniq_x)) \n  #this operation is two steps:\n  #first, match(x, uniq_x) turns the values in x into positions based on where they appear in uniq_x. For example: match(c(2, 2, 3), c(2, 3, 4)) returns 1 1 2.\n  #tabulate(...) then counts how many times each index appears:tabulate(c(1, 1, 2)) → 2 1 0\n  \n  mode_val &lt;- uniq_x[which.max(freq)]\n  #which.max(freq) gives the position of the most frequent value.\n  #uniq_x[...] pulls out the actual mode.\n  return(mode_val)\n}\n\nLet’s try it\n\nexample&lt;-c(1,2,3,3,3,4,6,7,6)\n\nget_mode(example)\n\n[1] 3"
  },
  {
    "objectID": "Legal393E_Notes/Lab4.html",
    "href": "Legal393E_Notes/Lab4.html",
    "title": "Legel 393E Spring 2025: Lab Meeting#4: Tutorial#5 Quiz Solutions",
    "section": "",
    "text": "This week, we will go over the solutions for Quiz 5 and some additional notes on how to explain regression results."
  },
  {
    "objectID": "Legal393E_Notes/Lab4.html#loading-the-necessary-datasets.",
    "href": "Legal393E_Notes/Lab4.html#loading-the-necessary-datasets.",
    "title": "Legel 393E Spring 2025: Lab Meeting#4: Tutorial#5 Quiz Solutions",
    "section": "Loading the necessary datasets.",
    "text": "Loading the necessary datasets.\n\n#create a vector contains the names of the needed packages, and assign a name \"needed.packages\" for it.\nneeded.packages&lt;-c('readxl','readr','haven','dplyr','broom','ggplot2', 'dotwhisker')\n\n#lapply() allows us to loop over and execute every element in the object \"needed.packages\".\nlapply(needed.packages, require, character.only = TRUE)\n\nLoading required package: readxl\n\n\nLoading required package: readr\n\n\nLoading required package: haven\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: broom\n\n\nLoading required package: ggplot2\n\n\nLoading required package: dotwhisker\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\n\n\nWe will need to load ANES and PSEO dataset. The other datasets (sleep, infert, faithful, chickwts, and cars) are preinstalled in R. For these pre-installed data, we can use data() to call up the datasets and load them in the R memory.\n\n#Check working directory\ngetwd()\n\n[1] \"C:/Users/donge/OneDrive/Desktop/GitHub/webpage/Legal393E_Notes\"\n\n\n\nLoading anes data\n\nanes&lt;- read_csv(\"https://raw.githubusercontent.com/RosemaryPang/Data-for-teaching/main/anes_601.csv\")   \n\nRows: 4271 Columns: 440\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (440): CASEID, v162002, v162003, v162004, v162005, v162006, v162007, v16...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(anes)   \n\n\n  \n\n\ndim(anes)\n\n[1] 4271  440\n\n#recode v162136x as Econ_Mobility, v162034a as Vote_16 and v162132 as Class.\n\nanes&lt;-anes%&gt;%\n  mutate(Econ_Mobility = case_when(\n         v162136x == 1 ~ \"Much Easier\",\n         v162136x == 2 ~ \"Moderately Easier\",\n         v162136x == 3 ~ \"Slightly Easier\",\n         v162136x == 4 ~ \"The Same\",\n         v162136x == 5 ~ \"Slightly Harder\",\n         v162136x == 6 ~ \"Moderately Harder\",\n         v162136x == 7 ~ \"Much Harder\"\n         )) %&gt;%\n  mutate(Vote_16 = case_when(\n         v162034a == 1 ~ \"Clinton\",\n         v162034a == 2 ~ \"Trump\",\n         v162034a == 3 ~ \"Johnson\",\n         v162034a == 4 ~ \"Stein\"\n         )) %&gt;%\n  mutate(Class = case_when(\n    v162132 == 1 ~ \"Lower Class\",\n    v162132 == 2 ~ \"Working Class\",\n    v162132 == 3 ~ \"Middle Class\",\n    v162132 == 4 ~ \"Upper Class\"\n  )) %&gt;%\n   mutate(party = case_when(\n         v162030 == 1 ~ \"Democratic\",\n         v162030 == 2 ~ \"Republican\"\n         )) %&gt;%\n  mutate(business = v162100) %&gt;%\n  mutate(liberals = v162097) %&gt;%\n  mutate(rich = v162105)\n\n\n\nLoading pseo data\n\npseo &lt;- read_csv(\"https://raw.githubusercontent.com/RosemaryPang/Data-for-teaching/main/pseo_601.csv\")\n\nRows: 18783 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): institution_name, deglevl, ciptitle, grad_cohort_label, state\ndbl (9): institution_id, deglevl_code, degcip_4dig, grad_cohort, year_postgr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(pseo)\n\n\n  \n\n\ndim(pseo)\n\n[1] 18783    14"
  },
  {
    "objectID": "Legal393E_Notes/Lab4.html#tutorial5-quiz-answers",
    "href": "Legal393E_Notes/Lab4.html#tutorial5-quiz-answers",
    "title": "Legel 393E Spring 2025: Lab Meeting#4: Tutorial#5 Quiz Solutions",
    "section": "Tutorial#5 Quiz Answers",
    "text": "Tutorial#5 Quiz Answers\n\nQuestion 1. There is a significant difference in the group means of feeling thermometer measure towards “Rich People” between Democrats and Republican respondents.\nAnswer: this question is about the ANES data. Since it is testing the “statistical difference” between groups, we know that we will need to conduct a t test. You can refer to the slides of Week6 on how to do two-sample t-test step by step.\nFirst, unlike the tutorial, I want to do an F-test to test whether the two samples have the same variance. We need to do this test before t-test because we need to determine what types of t-test we need to do (Student’s t for two groups have the SAME population variance (homoskedasticity); Welch’s t for otherwise).\n\nvar.test(rich~party, data = anes)\n\n\n    F test to compare two variances\n\ndata:  rich by party\nF = 0.58166, num df = 36, denom df = 20, p-value = 0.1537\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2520175 1.2271383\nsample estimates:\nratio of variances \n         0.5816633 \n\n\nThe p-value of the F-test is 0.1537, meaning that we cannot reject the null hypothesis that the two groups have the same variance. So, we should proceed with student’s t-test as tutorial teaches us.\n\nt.test(rich~party, var.equal = TRUE, data=anes) #\n\n\n    Two Sample t-test\n\ndata:  rich by party\nt = -0.54888, df = 56, p-value = 0.5853\nalternative hypothesis: true difference in means between group Democratic and group Republican is not equal to 0\n95 percent confidence interval:\n -123.60921   70.44061\nsample estimates:\nmean in group Democratic mean in group Republican \n                71.89189                 98.47619 \n\n\nAgian, the p-value is greater than 0.05 and we cannot reject the null hypothesis: there is no significant difference in the group means between Democrat and Republican respondents on the rich people. Interestingly, this result has been repeatedly confirmed by numerous studies, including a few recent published ones.\n\n\nQuestion 2. Relative to the baseline, a post-baccalaureate certificate is associated with significantly increased median earnings (p &lt; 0.05) among survey respondents in our regression model.\nAnswer:\n(Noted that since the question only asks you to investigate a bivariate relationship, so you can actually use either ANOVA or regression to investigate the statistical significance).\nThe difficult part of this study is to explain the regression results, not the method itself. Let’s run the codes first:\n\nmodel1&lt;-lm(p50_earnings ~ deglevl, data = pseo)\nsummary(model1)\n\n\nCall:\nlm(formula = p50_earnings ~ deglevl, data = pseo)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-55250 -13514  -3396   9121 271072 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                              40849.2      804.9  50.751  &lt; 2e-16\ndeglevlBaccalaureate                      7347.8      843.6   8.710  &lt; 2e-16\ndeglevlCertificate, &lt;1 year              -3550.2     1186.7  -2.992  0.00278\ndeglevlCertificate,1-2 years              -641.8     1345.4  -0.477  0.63334\ndeglevlDoctoral - Professional Practice  62507.5     1856.5  33.670  &lt; 2e-16\ndeglevlDoctoral - Research/Scholarship   38132.6     1187.9  32.101  &lt; 2e-16\ndeglevlMasters                           20700.5      981.0  21.101  &lt; 2e-16\ndeglevlPost-Bacc Certificate             14540.7     7216.1   2.015  0.04393\n                                           \n(Intercept)                             ***\ndeglevlBaccalaureate                    ***\ndeglevlCertificate, &lt;1 year             ** \ndeglevlCertificate,1-2 years               \ndeglevlDoctoral - Professional Practice ***\ndeglevlDoctoral - Research/Scholarship  ***\ndeglevlMasters                          ***\ndeglevlPost-Bacc Certificate            *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20280 on 9964 degrees of freedom\n  (8811 observations deleted due to missingness)\nMultiple R-squared:  0.2307,    Adjusted R-squared:  0.2302 \nF-statistic: 426.9 on 7 and 9964 DF,  p-value: &lt; 2.2e-16\n\n\nNoted the regression results: it looks very different from other regression results of two continous variables. For example:\n\ndata(mtcars)\nmodel2&lt;-lm(mpg ~ wt, data = mtcars)\nsummary(model2)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nWhy?\nBecause the independent variable of deglevl is categorical (an ordinal variable of degree level), not a continuous variable like wt (weight of a car). When interpreting the results of a categorical independent variable in an OLS linear regression, each coefficient represents the difference in the outcome variable between that category and the reference (baseline) category, which is automatically chosen by the software (usually the first alphabetically unless specified). The intercept gives the predicted value of the dependent variable for the reference group. Each coefficient tells us how much higher or lower the predicted value is for that group compared to the reference, holding all else constant.\nTherefore, the results of Model 1 show the effects of the specific IV value (degree level) in contrast to the baseline degree level. Recall Question#2: it is about comparing the “baseline” and the “post-baccalaureate certificate”. So what is the baseline degree level in this model? We will need to check the coding of deglevl:\n\ntypeof(pseo$deglevl)\n\n[1] \"character\"\n\n#opps, it seems like deglevel is coded by characters, not in numerical values. So we will have to convert it to factor (integer) so that we can present the different \"values\" of it.\n\npseo$deglevl &lt;- factor(pseo$deglevl)\ntypeof(pseo$deglevl)\n\n[1] \"integer\"\n\n#use levels() to present different values given the variable is labeled by characters.\n\nlevels(pseo$deglevl)\n\n [1] \"Associates\"                       \"Baccalaureate\"                   \n [3] \"Certificate, &lt;1 year\"             \"Certificate,1-2 years\"           \n [5] \"Certificate,2-4 years\"            \"Doctoral - Professional Practice\"\n [7] \"Doctoral - Research/Scholarship\"  \"Masters\"                         \n [9] \"Post-Bacc Certificate\"            \"Post-Masters Certificate\"        \n\n\nOk. Now we can see in our regression results, which category is missing? The missing category is the baseline category.\nSo here is how we explain the results (comparing post-baccalaureate certificate and associate degree):\n“Individuals whose highest degree is a Post-Baccalaureate Certificate earn, on average, $14,541 more (the coefficient) in median earnings than those in the baseline group (associate degree). The difference is statistically significant at the 0.05 level (p &lt; 0.05).”\nAdditional note: we can easily change the baseline level by using the relevel() function.\n\npseo$deglevl &lt;- factor(pseo$deglevl)  # Ensure it's a factor\npseo$deglevl &lt;- relevel(pseo$deglevl, ref = \"Baccalaureate\") #set Baccalaureate as the new baseline\n\n\nmodel1_bacc &lt;- lm(p50_earnings ~ deglevl, data = pseo)\nsummary(model1_bacc)\n\n\nCall:\nlm(formula = p50_earnings ~ deglevl, data = pseo)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-55250 -13514  -3396   9121 271072 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                              48196.9      252.7 190.693  &lt; 2e-16\ndeglevlAssociates                        -7347.8      843.6  -8.710  &lt; 2e-16\ndeglevlCertificate, &lt;1 year             -10898.0      907.9 -12.003  &lt; 2e-16\ndeglevlCertificate,1-2 years             -7989.5     1107.3  -7.216 5.76e-13\ndeglevlDoctoral - Professional Practice  55159.7     1691.9  32.603  &lt; 2e-16\ndeglevlDoctoral - Research/Scholarship   30784.9      909.5  33.849  &lt; 2e-16\ndeglevlMasters                           13352.8      615.1  21.707  &lt; 2e-16\ndeglevlPost-Bacc Certificate              7192.9     7175.5   1.002    0.316\n                                           \n(Intercept)                             ***\ndeglevlAssociates                       ***\ndeglevlCertificate, &lt;1 year             ***\ndeglevlCertificate,1-2 years            ***\ndeglevlDoctoral - Professional Practice ***\ndeglevlDoctoral - Research/Scholarship  ***\ndeglevlMasters                          ***\ndeglevlPost-Bacc Certificate               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20280 on 9964 degrees of freedom\n  (8811 observations deleted due to missingness)\nMultiple R-squared:  0.2307,    Adjusted R-squared:  0.2302 \nF-statistic: 426.9 on 7 and 9964 DF,  p-value: &lt; 2.2e-16\n\n\nOne more thing:\nYou may also realize I assign names for models (model 1, model 1.1, model 2), instead of just running the R functions and calling the results. There are two reasons why I do so: (1) you can type the model name to call the model without retyping the model specification and methods you apply to the model, this saves a lot of time especially if your model contains multiple variables; and (2) when we applying other functions to plot graphs (such as dot-whisker plot) and present results in a table (such as presenting multiple models in one table), calling models by their assigned names makes your coding easier and more tidy.\n\n\nQuestion 3. According to our bivariate regression model, median earnings (p50_earnings) for graduates increases by what amount each year after graduation, on average?\nAnswer: this is similar to the last question, but we are runnign a bivariate regression model with two continuous variables (earning and years after graduation).\n\nmodel3&lt;-lm(p50_earnings ~ year_postgrad, data = pseo)\n\nsummary(model3)\n\n\nCall:\nlm(formula = p50_earnings ~ year_postgrad, data = pseo)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-44670 -13861  -5236   8851 250178 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   39880.13     319.76  124.72   &lt;2e-16 ***\nyear_postgrad  2921.09      62.87   46.46   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20960 on 9970 degrees of freedom\n  (8811 observations deleted due to missingness)\nMultiple R-squared:  0.178, Adjusted R-squared:  0.1779 \nF-statistic:  2159 on 1 and 9970 DF,  p-value: &lt; 2.2e-16\n\n\nThe answer is 2,981.09.\n\n\nQuestion 4. According to our bivariate regression model, median earnings (p50_earnings) for graduates in the year after they graduate (year_postgrad == 0) is on average:\nAnswer: this is a simple one. So the above regression formula can be written in the following way: \\(earnings = \\alpha + \\beta*years\\), or \\(earnings = intercept + slope * years\\), thus \\(earnings = 39880.13 + 2921.09*years\\). If year_postgrad = 0, the predicted earning is $39,880.13.\n\n\nQuestion 5. Say we have two variables, X and Y. What function would calculate the correlation between those two variables?\nJust simple as cor(X, Y)."
  },
  {
    "objectID": "Legal393E_Notes/Lab4.html#additional-introduction-to-present-regression-results.",
    "href": "Legal393E_Notes/Lab4.html#additional-introduction-to-present-regression-results.",
    "title": "Legel 393E Spring 2025: Lab Meeting#4: Tutorial#5 Quiz Solutions",
    "section": "Additional Introduction to Present Regression Results.",
    "text": "Additional Introduction to Present Regression Results.\n\n1. Creating dot-whisker plots to visually present regression results.\nYou may remember we introduced a few graphs to visualize the results in week#10. Among them, the dot-whisker plot with the reference level (the dasheded line of 0) and the bands of the confidence interval for each variable is a great way to present regression results. In R, we can easily apply a function in the dotwhisker pacakge (an extension based on ggplot2) to do so:\n\nlibrary(dotwhisker)\nlibrary(broom)\n\n#Tidy the model\nm1_tidy &lt;- tidy(model1) #The tidy() function converts a model object (like lm, glm, etc.) into a clean, easy-to-read data frame — with one row per model term (e.g., intercept, predictors).\n\n\n#Relabel the variable so it looks clear. This step is optional, but strongly suggested, especially for continous/numerical values.\nlibrary(dplyr)\n\nm1_tidy &lt;- m1_tidy %&gt;%\n  mutate(term = recode(term,\n    \"(Intercept)\" = \"Baseline: Associate Degree\",\n    \"deglevlBaccalaureate\" = \"Bachelor's Degree\",\n    \"deglevlMasters\" = \"Master's Degree\",\n    \"deglevlDoctoral - Professional Practice\" = \"Doctoral (Professional)\",\n    \"deglevlDoctoral - Research/Scholarship\" = \"Doctoral (Research)\",\n    \"deglevlCertificate, &lt;1 year\" = \"Certificate (&lt;1 year)\",\n    \"deglevlCertificate,1-2 years\" = \"Certificate (1–2 years)\",\n    \"deglevlPost-Bacc Certificate\" = \"Post-Bacc Certificate\"\n  ))\n\n\n\n#Finally, plot the graph\ndwplot(m1_tidy, by_2sd = FALSE) +\n  theme_minimal() +\n  labs(\n    title = \"Model 1: Reference Group = Associate Degree\",\n    x = \"Coefficient Estimate (in dollars)\",\n    y = \"\",\n    caption = \"Each point shows the estimated difference in median earnings\"\n  ) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\")\n\n\n\n\n\n\n\n\nWe can also plot two models on the same graph:\n\n# Tidy both models\nmodel1_tidy &lt;- tidy(model1) %&gt;% mutate(model = \"Reference: Associate\")\nmodel2_tidy &lt;- tidy(model1_bacc) %&gt;% mutate(model = \"Reference: Baccalaureate\")\n\n# Combine models\ncombined_models &lt;- bind_rows(model1_tidy, model2_tidy)\n\n# Plot dot-whisker plot\ndwplot(combined_models, by_2sd = FALSE) +\n  theme_minimal() +\n  labs(\n    title = \"Effect of Degree Level on Median Earnings\",\n    x = \"Coefficient Estimate (in dollars)\",\n    y = \"\",\n    caption = \"Based on OLS regressions with two different reference groups\"\n  ) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\")\n\n\n\n\n\n\n\n\nYou can check out the user mannual here to see how to cutomize the colors, labels, position of legends, and adding a reference line.\n\n\n2. Visualizing the effect of degree level on predicted earnings (Marginal effects)\nA marginal effect tells us how the expected value of the dependent variable (e.g., earnings) changes when we change one independent variable, while holding all others constant.\n\nFor categorical variables (like degree level), it shows the expected difference in outcomes between each category and a baseline.\nFor continuous variables, it represents the slope — the expected change in the outcome for each unit increase in the predictor.\n\nIn general, presenting a marginal effect plot allows us to see the differences more intuitively. This is especially helpful when we try to interpret the results of models with binary or catetgorical dependent variables.\nLet’s take a look at the pseo example of earning and years of graduation.\n\n#Step#1: run the linear regression (we already ran it above)\n# model3 &lt;- lm(p50_earnings ~ year_postgrad, data = pseo)\n\n\n#step#2: create a prediction datase contains a sequence of values for year_postgrad (e.g., 0, 1, 2, ..., 10) \nnewdata &lt;- data.frame(year_postgrad = seq(min(pseo$year_postgrad, na.rm = TRUE),\n                                          max(pseo$year_postgrad, na.rm = TRUE),\n                                          by = 1)) #we specify the interval of year is 1.\n\n#Step#3: Use predict() to Get Fitted Values and Confidence Intervals \npredicted &lt;- predict(model3, newdata = newdata, interval = \"confidence\")\npredicted_df &lt;- cbind(newdata, as.data.frame(predicted))\n\n\n\n#Step#4: Plot it with ggplot2 using scatterplot geom_point():\nggplot(predicted_df, aes(x = year_postgrad, y = fit)) +\n  #fit = the predicted average earnings (based on your model)\n  geom_line(color = \"blue\")  +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.2)  +\n  #lwr and upr = lower and upper bounds of the 95% confidence interval\n  labs(\n    title = \"Predicted Median Earnings by Years Post-Graduation\",\n    x = \"Years After Graduation\",\n    y = \"Predicted Median Earnings (USD)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSome of you may ask why don’t use geom_point() ? The reason is that the years of graduation is a continous variable with fixed interval (each gap is a full year, not fractional like earnings (1.7, 300.2); or we call this a discrete variable). If we plot it using geom_point() plot each predicted value as a dot (instead of a connected line). And it emphasizes the individual predictions more than the trend.\n\nggplot(predicted_df, aes(x = year_postgrad, y = fit)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.3, color = \"blue\") +\n  labs(\n    title = \"Predicted Median Earnings by Years Post-Graduation\",\n    x = \"Years After Graduation\",\n    y = \"Predicted Median Earnings (USD)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3. Explaining logistic regression (Week 9 and Week 10)\nLet’s take a look at an example of the dataset which we didn’t cover briefly in the class.\n\nlibrary(mlbench)\ndata(\"PimaIndiansDiabetes\", package = \"mlbench\")\npima&lt;-PimaIndiansDiabetes #use the abbreiviation\nhead(pima)\n\n\n  \n\n\nsummary(pima)\n\n    pregnant         glucose         pressure         triceps     \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    insulin           mass          pedigree           age        diabetes \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780   Min.   :21.00   neg:500  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437   1st Qu.:24.00   pos:268  \n Median : 30.5   Median :32.00   Median :0.3725   Median :29.00            \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719   Mean   :33.24            \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262   3rd Qu.:41.00            \n Max.   :846.0   Max.   :67.10   Max.   :2.4200   Max.   :81.00            \n\n\nThe data set PimaIndiansDiabetes2 contains a corrected version of the original data set. While the UCI repository index claims that there are no missing values, closer inspection of the data shows several physical impossibilities, e.g., blood pressure or body mass index of 0. In PimaIndiansDiabetes2, all zero values of glucose, pressure, triceps, insulin and mass have been set to NA, see also Wahba et al (1995) and Ripley (1996).\nSuppose we are studying how glucose level and body mass affect the probability of getting diabete, we can model the regression below:\n\nmodel_diabete&lt;-glm(diabetes ~ glucose + mass, data = pima,\nfamily = binomial(link = \"logit\"))\n\nsummary(model_diabete)\n\n\nCall:\nglm(formula = diabetes ~ glucose + mass, family = binomial(link = \"logit\"), \n    data = pima)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -7.515639   0.605236 -12.418  &lt; 2e-16 ***\nglucose      0.035169   0.003289  10.694  &lt; 2e-16 ***\nmass         0.076334   0.013338   5.723 1.05e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 771.40  on 765  degrees of freedom\nAIC: 777.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nNote: since we are doing log-odd transformation for the dependent variable to fit a lienar regression model, we cannot use the same way to explain the coefficients of the logistic regression.\nIn other words, what the result presented here is corresponding to this model:\n\n\n\nAnd we want to get \\(p\\).\n\n\n\nSuppose someone has glucose = 120 and mass = 35. To calculate the probability of getting diabete, we can do manually calculation by\n\n\n\n\n\n\nSo how do we explain this? it means that: “For an individual with a glucose level of 120 and BMI of 35, the model predicts that they have a 34.1% chance of having diabetes.”\nWe can also do this in R:\n\nnewdata &lt;- data.frame(glucose = 120, mass = 35)\n\npredict(model_diabete, newdata = newdata, type = \"response\")\n\n        1 \n0.3489477 \n\nnewdata$predicted_prob &lt;- predict(model_diabete, newdata = newdata, type = \"response\")\n\nprint(newdata)\n\n  glucose mass predicted_prob\n1     120   35      0.3489477"
  },
  {
    "objectID": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html",
    "href": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html",
    "title": "Legel 393E Spring 2025: Group Project Lab(1)",
    "section": "",
    "text": "#create a vector contains the names of the needed packages, and assign a name \"needed.packages\" for it.\nneeded.packages&lt;-c('readxl','readr','haven','dplyr','broom','ggplot2','dotwhisker')\n\n#lapply() allows us to loop over and execute every element in the object \"needed.packages\".\nlapply(needed.packages, require, character.only = TRUE)\n\nLoading required package: readxl\n\n\nLoading required package: readr\n\n\nLoading required package: haven\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: broom\n\n\nLoading required package: ggplot2\n\n\nLoading required package: dotwhisker\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\nassign a name “needed.packages” for it.\nneeded.packages&lt;-c(‘readxl’,‘readr’,‘haven’,‘dplyr’,‘broom’,‘ggplot2’,‘dotwhisker’)\n#lapply() allows us to loop over and execute every element in the object “needed.packages”.\nlapply(needed.packages, require, character.only = TRUE)"
  },
  {
    "objectID": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#load-the-dataset",
    "href": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#load-the-dataset",
    "title": "Legel 393E Spring 2025: Group Project Lab(1)",
    "section": "Load the dataset",
    "text": "Load the dataset\nBefore we explore the data, we need to import the survey responses (CSV file) into R.\nMake sure the file simulated_legal393_survey_responses.csv is in the same folder as this .qmd file.\n\n# Read CSV file\ndata &lt;- read.csv(\"simulated_legal393_survey_responses.csv\")\n\n# Preview first few rows\nhead(data)"
  },
  {
    "objectID": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#part-1-understanding-the-dataset",
    "href": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#part-1-understanding-the-dataset",
    "title": "Legel 393E Spring 2025: Group Project Lab(1)",
    "section": "Part 1: Understanding the Dataset",
    "text": "Part 1: Understanding the Dataset\nBefore we clean or analyze any data, we need to understand what the dataset looks like. This step includes:\n\nChecking the overall structure of the data.\nViewing variable names and sample entries.\nEnsure values are stored in the expected format (e.g., text, numbers).\n\n\nglimpse(data)       # View the dataset structure (variables, types, and first few values)\n\nRows: 250\nColumns: 45\n$ user_id                    &lt;chr&gt; \"2e750639\", \"2af57c72\", \"1fb7bdb8\", \"d372a2…\n$ consent                    &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\"…\n$ gender                     &lt;chr&gt; \"Female\", \"Non-Binary\", \"Male\", \"Female\", \"…\n$ birth_year                 &lt;int&gt; 1994, 1997, 1978, 1992, 1988, 1964, 1972, 1…\n$ ethnicity                  &lt;chr&gt; \"Black\", \"Other\", \"White\", \"Black\", \"Hispan…\n$ hispanic_origin            &lt;chr&gt; \"Hispanic or Latino\", \"Hispanic or Latino\",…\n$ education                  &lt;chr&gt; \"Bachelor’s degree\", \"Graduate or professio…\n$ employment                 &lt;chr&gt; \"Other\", \"Part-time\", \"Part-time\", \"Homemak…\n$ income                     &lt;chr&gt; \"$100k-$149k\", \"$150k+\", \"$25k-$49k\", \"$150…\n$ residency_state            &lt;chr&gt; \"MA\", \"CA\", \"FL\", \"FL\", \"NY\", \"NY\", \"NY\", \"…\n$ residency_type             &lt;chr&gt; \"Rural\", \"Urban\", \"Suburban\", \"Suburban\", \"…\n$ citizenship                &lt;chr&gt; \"Naturalized\", \"Other\", \"Visa\", \"Visa\", \"Ot…\n$ social_media               &lt;chr&gt; \"YouTube\", \"Facebook\", \"TikTok\", \"Reddit\", …\n$ political_ideology         &lt;chr&gt; \"Somewhat Liberal\", \"Very Conservative\", \"S…\n$ party_affiliation          &lt;chr&gt; \"Republican\", \"Strong Democrat\", \"Republica…\n$ vote_2020                  &lt;chr&gt; \"Republican\", \"Other\", \"Did not vote\", \"Dem…\n$ criminal_record            &lt;chr&gt; \"Prefer not to disclose\", \"No\", \"Other\", \"Y…\n$ abortion_legality          &lt;chr&gt; \"Illegal most\", \"Unsure\", \"Legal all\", \"Leg…\n$ med_abortion_legal         &lt;chr&gt; \"Not sure\", \"Legal\", \"Not sure\", \"Illegal\",…\n$ abortion_access_difficulty &lt;chr&gt; \"Very difficult\", \"Somewhat difficult\", \"Ve…\n$ abortion_access_should_be  &lt;chr&gt; \"About the same\", \"Harder\", \"Easier\", \"Abou…\n$ immig_pop_size             &lt;int&gt; 4, 4, 7, 6, 2, 6, 1, 4, 3, 5, 7, 5, 4, 1, 4…\n$ immig_policy_adequacy      &lt;int&gt; 4, 6, 6, 7, 3, 2, 5, 5, 7, 1, 6, 1, 6, 2, 2…\n$ immig_policy_impact        &lt;int&gt; 7, 2, 1, 6, 4, 1, 4, 6, 5, 5, 5, 3, 5, 1, 6…\n$ immig_crime_change         &lt;chr&gt; \"Increase\", \"Increase\", \"Same\", \"Decrease\",…\n$ immig_safety_feeling       &lt;chr&gt; \"Somewhat safer\", \"Much more dangerous\", \"S…\n$ immig_belief_1             &lt;chr&gt; \"Neutral\", \"Agree\", \"Disagree\", \"Disagree\",…\n$ immig_belief_2             &lt;chr&gt; \"Disagree\", \"Strongly agree\", \"Strongly agr…\n$ immig_belief_3             &lt;chr&gt; \"Strongly agree\", \"Neutral\", \"Neutral\", \"St…\n$ immig_belief_4             &lt;chr&gt; \"Strongly disagree\", \"Agree\", \"Strongly agr…\n$ speech_express_likelihood  &lt;int&gt; 3, 7, 2, 6, 4, 5, 5, 1, 5, 2, 10, 2, 1, 3, …\n$ speech_backlash_likelihood &lt;int&gt; 5, 2, 6, 10, 7, 10, 4, 8, 1, 9, 5, 1, 8, 7,…\n$ scotus_fairness            &lt;int&gt; 2, 5, 1, 2, 10, 10, 8, 9, 2, 3, 4, 9, 2, 2,…\n$ scotus_trust               &lt;int&gt; 3, 5, 5, 8, 10, 9, 5, 4, 9, 9, 3, 5, 4, 1, …\n$ scotus_politics_influence  &lt;chr&gt; \"Yes\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Yes\"…\n$ trust_us_courts            &lt;chr&gt; \"Not much\", \"Moderate\", \"None\", \"Great deal…\n$ trust_io                   &lt;chr&gt; \"None\", \"Some\", \"Great deal\", \"Some\", \"Some…\n$ importance_us_io           &lt;chr&gt; \"Somewhat\", \"Very\", \"Very\", \"Extremely\", \"V…\n$ us_intervene               &lt;chr&gt; \"Only if interest\", \"Only if interest\", \"Ye…\n$ icc_familiarity            &lt;chr&gt; \"Never heard\", \"Never heard\", \"Somewhat\", \"…\n$ icc_reliable               &lt;chr&gt; \"Strongly agree\", \"Strongly disagree\", \"Str…\n$ icc_fair                   &lt;chr&gt; \"Agree\", \"Strongly agree\", \"Strongly agree\"…\n$ icc_effective              &lt;chr&gt; \"Strongly disagree\", \"Agree\", \"Disagree\", \"…\n$ icc_bias                   &lt;chr&gt; \"Neutral\", \"Somewhat unbiased\", \"Extremely …\n$ manip_check                &lt;chr&gt; \"AI in campaign finance\", \"AI in campaign f…\n\ndim(data)           # View numbers of rows (number of unique respondents) and columns (number of questions)\n\n[1] 250  45\n\nnames(data)         # Print the names of all variables\n\n [1] \"user_id\"                    \"consent\"                   \n [3] \"gender\"                     \"birth_year\"                \n [5] \"ethnicity\"                  \"hispanic_origin\"           \n [7] \"education\"                  \"employment\"                \n [9] \"income\"                     \"residency_state\"           \n[11] \"residency_type\"             \"citizenship\"               \n[13] \"social_media\"               \"political_ideology\"        \n[15] \"party_affiliation\"          \"vote_2020\"                 \n[17] \"criminal_record\"            \"abortion_legality\"         \n[19] \"med_abortion_legal\"         \"abortion_access_difficulty\"\n[21] \"abortion_access_should_be\"  \"immig_pop_size\"            \n[23] \"immig_policy_adequacy\"      \"immig_policy_impact\"       \n[25] \"immig_crime_change\"         \"immig_safety_feeling\"      \n[27] \"immig_belief_1\"             \"immig_belief_2\"            \n[29] \"immig_belief_3\"             \"immig_belief_4\"            \n[31] \"speech_express_likelihood\"  \"speech_backlash_likelihood\"\n[33] \"scotus_fairness\"            \"scotus_trust\"              \n[35] \"scotus_politics_influence\"  \"trust_us_courts\"           \n[37] \"trust_io\"                   \"importance_us_io\"          \n[39] \"us_intervene\"               \"icc_familiarity\"           \n[41] \"icc_reliable\"               \"icc_fair\"                  \n[43] \"icc_effective\"              \"icc_bias\"                  \n[45] \"manip_check\"               \n\n\n\nWhat to look for:\n\nAre demographic variables like gender, education, and birth_year stored as characters or numbers?\nIs the format of each variable suitable for statistical analysis?"
  },
  {
    "objectID": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#part-2-data-cleaning-and-recoding",
    "href": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#part-2-data-cleaning-and-recoding",
    "title": "Legel 393E Spring 2025: Group Project Lab(1)",
    "section": "Part 2: Data Cleaning and Recoding",
    "text": "Part 2: Data Cleaning and Recoding\nThis section focuses on:\n\nReformatting responses so they are usable in statistical models.\nTurning text into numbers (especially for ordinal variables).\nRecoding binary and categorical variables\nHandling missing or ambiguous responses.\n\n\n2.1 Recode Character Responses → Numeric\nMany variables use text responses (e.g., “Urban”, “Suburban”) but have an implied order or coding. We must convert these into numeric values for summary statistics or modeling.\n\nExample 1: Recode birth_year into age\n\n#always check the original column/variable:\nsummary(data$birth_year)#for continous variables\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1960    1972    1986    1985    1998    2010 \n\n#table(data$birth_year)#use table for ordinal or nominal variables\n\ndata &lt;- data %&gt;%\n  mutate(age = 2025 - as.numeric(birth_year)) \n#in this line, we create a new column/variable called \"age\" by using 2025 minus the birth year variable.\n#as.numeric() ensures R treats the values of the specific variables \"numeric\", so that we can conduct math operation.\n\n\nsummary(data$age) #Always inspect your new variable after you recode or create a new one!\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.00   27.25   39.00   40.14   53.00   65.00 \n\n\nWe can see that the age values are within a reasonable range (e.g., 15–80). There are no obvious errors or NAs created during conversion. This looks good.\n\n\nExample 2: Recode residency_type into numeric categories based on the original corresponding values/codes of the responses.\n\ndata &lt;- data %&gt;%\n  mutate(residency_type_num = case_when(\n    residency_type == \"Urban\" ~ 1,\n    residency_type == \"Suburban\" ~ 2,\n    residency_type == \"Rural\" ~ 3,\n    TRUE ~ 4\n  ))\n\ntable(data$residency_type_num)\n\n\n 1  2  3 \n93 85 72 \n\n\n\nThis recoding helps when we want to use residency_type as a predictor or control variable in a model. By assigning numbers, we can group or summarize responses more easily.\n\n\n\nIn-class Exercise A\nEach of you, choose one variable from the list below and do the following:\n\nLook at the response categories (from urvey questionnaire (in Word) or the survey project on Qualtrics).\nWrite your own recoding from character → numeric. Use the mutate() + case_when() pattern, or other pattern or codes you want (for example, ifelse())..\nExplain your reasoning (you will need to do this in your final group project writing).\n\nSuggested variables:\n\ngender\neducation\ncitizenship\npolitical_ideology\nparty_affiliation\nvote_2020\ncriminal_record\n\nUse the mutate() + case_when() pattern.\n\n\n\n2.2 Recode to Binary Variables\nSome research questions need yes/no answers or binary contrasts. We create binary variables from:\n\nNominal categories (e.g., “White” vs. Others)\nOrdinal categories (e.g., “College degree or higher” vs. not)\n\n\nExample 3: Nominal → Binary: Ethnicity — White = 1, Others = 0\nThe mutate() + case_when() approach:\n\ndata &lt;- data %&gt;%\n  mutate(ethnicity_white = case_when(\n    ethnicity == \"White\" ~ 1,\n    TRUE ~ 0 #everyone else (including NA) is assigned 0\n  ))\n\ntable(data$ethnicity_white)\n\n\n  0   1 \n207  43 \n\n\nA simpler and more straight forward method: ifelse()\n\ndata &lt;- data %&gt;%\n  mutate(white = ifelse(ethnicity == \"White\", 1, 0))\n\nsummary(data$white) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   0.172   0.000   1.000 \n\ntable(data$white) #just checking\n\n\n  0   1 \n207  43 \n\n\n\n\nExample 4: Ordinal → Binary: Education — Bachelor’s degree or higher = 1, others = 0\nThe mutate() + case_when() approach:\n\ndata &lt;- data %&gt;%\n  mutate(edu_collegeplus = case_when(\n    education %in% c(\"Bachelor’s degree\", \"Graduate or professional degree\") ~ 1, # The %in% operator in R checks whether elements of a vector belong to a specified set of values. It is usually used with strings (characters, words, phrases, etc.)\n    TRUE ~ 0 #everyone else (including NA) is assigned 0\n  ))\n\ntable(data$edu_collegeplus) #Confirm logic worked\n\n\n  0   1 \n166  84 \n\n\nA simpler and more straight forward method would do the same thing: ifelse()\n\ndata &lt;- data %&gt;%\n  mutate(edu_collegeplus = ifelse(\n    education %in% c(\"Bachelor’s degree\", \"Graduate or professional degree\"), 1, 0\n  ))\n\ntable(data$edu_collegeplus)\n\n\n  0   1 \n166  84 \n\n\nWhy this matter? Creating binary variables is important when testing hypotheses like “Are college-educated individuals more likely to support Policy X?” If your experiment contains only two conditions, you can think of recoding your independent variable in such a way.\n\n\n\nIn-class Exercise B\nUsing the same variable you chose in Exercise A:\n\nRecode that variable into a binary format (0/1).\nDecide what counts as 1 (e.g., Female? Democrat? High education?).\n\nYou may work and compare your logic with other students in your group.\n\n\n\n2.3 Handling “Prefer not to answer” or Missing Values\nBefore we decide how to handle missing responses like \"Prefer not to disclose\" or NA, let’s review three types of missing data in statistics:\n\nMissing Data: Types and Methods of Handling Them\n\n\n\n\n\n\n\n\nName\nDescription\nExample\nCol5\n\n\n\n\nMissing Completely at Random (MCAR)\nThe missingness is unrelated to any values, observed or unobserved.\nA random technical glitch skips a question.\nDrop rows, or use mean/median/mode imputation.\n\n\nMissing at Random (MAR)\nMissingness is related to observed data, but not the value itself.\nOlder people are more likely to skip income question.\nUse imputation based on other variables (e.g., regression, stratified mean using age, edu, employment to predict income).\n\n\nMissing Not at Random (MNAR)\nMissingness is related to the missing value itself.\nPeople with criminal records are more likely to choose “Prefer not to disclose.”\nCannot be fixed by simple imputation—requires careful modeling or logic-based assumptions.\n\n\n\n\nApplication to Our Data:\nLet’s apply this to a real variable: criminal_record.\nThe response of “Prefer not to disclose” might reflect MNAR — people with a criminal record may be more likely to refuse answering. Based on this assumption, That means we shouldn’t treat it as just missing — we can logically recode it as “Yes” if we want to be conservative.\n\n#the original coding of criminal record: \ntable(data$criminal_record)\n\n\n                    No                  Other Prefer not to disclose \n                    52                     62                     65 \n                   Yes \n                    71 \n\n#recode:\ndata &lt;- data %&gt;%\n  mutate(criminal_record_clean = case_when(\n    criminal_record == \"Prefer not to disclose\" ~ \"Yes\",\n    TRUE ~ criminal_record\n  ))\n\n#confirm recoding variable:\ntable(data$criminal_record_clean)\n\n\n   No Other   Yes \n   52    62   136 \n\n\n\nFor other variables (e.g., education), missingness may be MAR or MCAR, and we can safely fill it with mode, mean or median. In the following case, because education is an ordinal variable, we cannot directly use mean or mean. We use mode to replace “prefer not to say”.\n\n\n#write a function to get mode\n\nget_mode &lt;- function(x) {\n  x &lt;- na.omit(x)  # Remove NA values\n  uniq_x &lt;- unique(x) #This gets all the unique values in x\n  freq &lt;- tabulate(match(x, uniq_x)) \n  #this operation is two steps:\n  #first, match(x, uniq_x) turns the values in x into positions based on where they appear in uniq_x. For example: match(c(2, 2, 3), c(2, 3, 4)) returns 1 1 2.\n  #tabulate(...) then counts how many times each index appears:tabulate(c(1, 1, 2)) → 2 1 0\n  \n  mode_val &lt;- uniq_x[which.max(freq)]\n  #which.max(freq) gives the position of the most frequent value.\n  #uniq_x[...] pulls out the actual mode.\n  return(mode_val)\n}\n\nmode_edu &lt;- get_mode(data$education)\n\n#originally education\ntable(data$education)\n\n\n Associates or technical degree               Bachelor’s degree \n                             63                              40 \nGraduate or professional degree      High school diploma or GED \n                             44                              40 \n        Some college, no degree        Some high school or less \n                             22                              41 \n\n#filling prefer not to say with mode of education\ndata &lt;- data %&gt;%\n  mutate(education_filled = case_when(\n    education == \"Prefer not to say\" ~ mode_edu,\n    TRUE ~ education))\n\n#check the filled education variable\ntable(data$education_filled)\n\n\n Associates or technical degree               Bachelor’s degree \n                             63                              40 \nGraduate or professional degree      High school diploma or GED \n                             44                              40 \n        Some college, no degree        Some high school or less \n                             22                              41 \n\n\n\n\n\nReview: How We Handle Missing Data in This Class?\n\nFor MAR and MNAR situations (like criminal_record), we have not learned advanced imputation techniques yet, such as multiple imputation or model-based imputation.\nTherefore, we will use simple strategies like:\n\nLogic-based recoding (e.g., treating “Prefer not to disclose” as “Yes”)\nMean / mode / median imputation for numeric or categorical variables\n\n\n\nRecommendation for Your Group Projects\nIn the final project, I will help you to handle the missing data in the standard questions (demographic, socio-economic status, and personal-others). When you analyze your group’s experimental variables (IVs and DVs), please:\n\nRun your model using only complete cases (i.e., drop any rows with missing data using na.omit() or drop_na()).\nThen, run the same model again after applying simple imputations (like using mean() or mode() to fill missing values).\nCompare the results, and note whether your conclusions change. This will help you reflect on how missing data affects analysis."
  },
  {
    "objectID": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#part-3-descriptive-statistics-and-plots",
    "href": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#part-3-descriptive-statistics-and-plots",
    "title": "Legel 393E Spring 2025: Group Project Lab(1)",
    "section": "Part 3: Descriptive Statistics and Plots",
    "text": "Part 3: Descriptive Statistics and Plots\nOnce your data is cleaned and recoded, the next step is to explore and summarize it. This section reviews how to:\n\nGenerate basic summary statistics\nCreate simple visualizations to display data distribution of a variable (make sure you check the previous lecture note to decide what types of graph you should choose to plot).\nDescribe the data distribution based on the summary statistics and the graphs. For example, in the third graph below, are there more Democrats or Republicans in the sample? Is there a strong skew in political identity?\nExplore relationships between variables\n\n\n3.1 Univariate Statistics (Single Variable)\n\nExample 1: Age (numeric variable);\nSince age is a continuous variable, we can use boxplot to present its min, max, mean, median, and quartiles. We can also do histogram.\n\n#summary statistics\nsummary(data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.00   27.25   39.00   40.14   53.00   65.00 \n\n#boxplot to present distribution\nggplot(data, aes(x = \"\", y = age)) + \n geom_boxplot() +\n labs(y = \"Age\") +\n ggtitle(\"Distribution of Respondent Age\")\n\n\n\n\n\n\n\n\n\n\nExample 2: Gender (nominal)\nSince gender is a nominal variable, we can use barplot to present the proportion of each gender group. We can also use pie-chart to do so.\n\nprop.table(table(data$gender))#use prop.table to show proportion of different groups of a  nominal variable \n\n\n    Female       Male Non-Binary      Other \n     0.192      0.280      0.268      0.260 \n\n#plotting barplot\nggplot(data, aes(x = gender)) +\n  geom_bar() +\n  labs(x = \"Gender\", y = \"Count\") +\n  ggtitle(\"Gender Breakdown in Sample\")\n\n\n\n\n\n\n\n\n\n\nExample 3: Party affiliation (ordinal)\nSince Party affiliation is an ordinal variable (7-point scale), we can use a barplot to present the proportion of each gender group. If an ordinal variable has more than 10 values, we can also choose a histogram. But do not use a boxplot.\n\n#the following code is crucial for handling ordered categorical variables in R when you want to treat them as numbers (e.g., for plotting or modeling).\n\ndata$party_numeric &lt;- as.numeric(factor(data$party_affiliation, #step2: as.numeric(factor): converts that ordered factor into numbers 1 to 7, matching the ideological spectrum.\n  levels = c(\"Strong Republican\", \"Republican\", \"Lean Republican\",\n             \"Independent\", \"Lean Democrat\", \"Democrat\", \"Strong  Democrat\") #step1: levels = c: it first converts a text variable (party_affiliation) into a factor with explicit order based on political ideology (from conservative to liberal).\n))\n\n#However, my personal practice is that I would recode all these variables by assigining responses with numerical values in Part#1.\n\n#If you already do so \n\n\nggplot(data, aes(x = party_numeric)) +\n  geom_bar() +\n  labs(x = \"Party Affiliation (1 = Strong R → 7 = Strong D)\", y = \"Count\") +\n  ggtitle(\"Party Identification of Survey Respondents\")\n\nWarning: Removed 29 rows containing non-finite outside the scale range\n(`stat_count()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 Bivariate Plots\nLet’s examine how two variables relate to each other.\n\nExample 4: Party affiliation (numeric) vs Age\n\nggplot(data, aes(x = factor(party_numeric), y = age)) +\n  geom_boxplot() +\n  labs(x = \"Party Affiliation\", y = \"Age\") +\n  ggtitle(\"Age Differences by Party Affiliation\")\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise C\nChoose any two variables from your dataset:\n\nOne numeric (e.g., age, party_numeric, speech_express_likelihood)\nOne categorical (e.g., gender, education, ethnicity_white)\n\nFor each variable:\n\nGenerate summary statistics using summary() or table().\nMake a plot using ggplot2 (e.g., boxplot, bar chart, or histogram).\n(Optional) Write down one sentence interpreting what you observe. You will need to do this in your final group report.\n\n✔️ Tip: Use labs() to label your plot axes meaningfully."
  },
  {
    "objectID": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#final-section-group-project-lab-2-preview",
    "href": "Legal393E_Notes/legal393_data_cleaning_guide_v2.html#final-section-group-project-lab-2-preview",
    "title": "Legel 393E Spring 2025: Group Project Lab(1)",
    "section": "Final Section: Group Project Lab (2) Preview",
    "text": "Final Section: Group Project Lab (2) Preview\nFor your group project, you will work with your own experimental questions. This week, we are practicing using simulated data. But you can still start writing the codes.\nEach group should:\n\nIdentify your experimental variables (the dependent variable questions that measure the attitude and behavior in your experiment) from the survey\nWrite the code to recode your group’s DVs in a proper form using mutate() + case_when().\nDon’t worry about the demographic, socioeconomic and personal-other variables, I will clean them for you.\nApply the same cleaning and checking habits from today:\n\n\n-   Try out the codes for summary stats\n\n-   Write the codes for plotting distribution graphs. \n\n\nSave your codes with the .R file.\n\nOn Friday (real data release day):\nYou will reuse your cleaned code on the real dataset.\n\nThen, you will perform:\n\nSimple regressions or t-tests (depending on design)\nMultiple regression including all control variables.\nHypothesis testing and results interpretation"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Yongdong Chen",
    "section": "",
    "text": "Welcome to my website! I am a PhD student in Operations Management in UMass Amherst. My research interest is sustainable operations management.\nYou can find more details in my Curriculum Vitae(FileSample)."
  },
  {
    "objectID": "index.html#project",
    "href": "index.html#project",
    "title": "Yongdong Chen",
    "section": "",
    "text": "I have worked on several data-driven projects. Please verify the Project page for more details."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Academic Projects",
    "section": "",
    "text": "This is a data-driven project exploring the trends of sustainability reporting and external auditing among S&P 500 companies.\nYou can view the interactive dashboard for this project here: Click to Open Dashboard\nProject Description: The project analyzes data from the past decade to understand how major corporations are adopting sustainability practices. The visualization highlights a significant “reporting boom” and a growing preference for externally audited reports, suggesting a shift towards higher transparency and credibility in the corporate sector."
  },
  {
    "objectID": "project.html#project-1-这里写你的项目名称",
    "href": "project.html#project-1-这里写你的项目名称",
    "title": "Sample Projects",
    "section": "",
    "text": "This is a data-driven project exploring (这里写一句简短的描述，比如研究了什么问题).\nYou can view the interactive dashboard for this project here: Dashboard Link\nBrief description no more than 100 words."
  },
  {
    "objectID": "project.html#other-works",
    "href": "project.html#other-works",
    "title": "Academic Projects",
    "section": "Other Works",
    "text": "Other Works\n\nComing Soon"
  },
  {
    "objectID": "flexboard.html#column",
    "href": "flexboard.html#column",
    "title": "Sustainability Dashboard",
    "section": "",
    "text": "if(nrow(plot_data) &gt; 0) {\n  \n  ggplot(plot_data, aes(x = Year, y = Count, fill = fct_rev(AuditorLabel))) +\n    geom_col(width = 0.65) + \n    \n    scale_fill_manual(values = c(\"Not Audited\" = \"#A5D6A7\", \"Audited\" = \"#2E8B57\")) +\n    \n    scale_x_continuous(breaks = seq(min(plot_data$Year), max(plot_data$Year), 1)) +\n    \n    scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n    \n    theme_minimal(base_size = 14) +\n    theme(\n      legend.position = \"top\", \n      legend.justification = \"left\",\n      legend.title = element_blank(),\n      \n      panel.grid.major.x = element_blank(),\n      panel.grid.minor = element_blank(),\n      \n      plot.title = element_text(face = \"bold\", size = 18),\n      plot.subtitle = element_text(size = 12, color = \"grey30\")\n    ) +\n    \n    labs(\n      title = \"The Sustainability Reporting Boom\",\n      subtitle = \"Number of S&P 500 firms publishing sustainability reports\",\n      x = \"\",\n      y = \"Number of Companies\"\n    )\n    \n} else {\n  print(\"Error: No data found.\")\n}"
  },
  {
    "objectID": "flexboard.html#column-1",
    "href": "flexboard.html#column-1",
    "title": "Sustainability Dashboard",
    "section": "Column",
    "text": "Column\n\nDescription\n\nObservation: The chart shows the trend of firms publishing sustainability report and adopting external auditing over the years.\nTrend: We can observe that the number of “Audited” firms (Dark Green) is increasing compared to “Not Audited” firms.\nConclusion: This suggests a growing emphasis on sustainability verification in the corporate sector."
  },
  {
    "objectID": "project.html#project-1-sustainability-reporting-analysis",
    "href": "project.html#project-1-sustainability-reporting-analysis",
    "title": "Academic Projects",
    "section": "",
    "text": "This is a data-driven project exploring the trends of sustainability reporting and external auditing among S&P 500 companies.\nYou can view the interactive dashboard for this project here: Click to Open Dashboard\nProject Description: The project analyzes data from the past decade to understand how major corporations are adopting sustainability practices. The visualization highlights a significant “reporting boom” and a growing preference for externally audited reports, suggesting a shift towards higher transparency and credibility in the corporate sector."
  }
]